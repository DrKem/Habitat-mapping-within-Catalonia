import numpy as np
!pip install ultralytics

SPECIES IDENTIFICATION
This code counts the number of persons or objects detected in each video frame.

from ultralytics import YOLO

# Load a model
model = YOLO('yolov8n.pt')  # load an official model
#model = YOLO('path/to/best.pt')  # load a custom model

# Predict with the model
results = model('/content/drive/MyDrive/Computer Vision/startic 3lvx enhanced.mp4', stream = True)  # predict on an image

!pip install opencv-python



from ultralytics import YOLO
import cv2
from google.colab.patches import cv2_imshow

# Load the YOLOv8 model
model = YOLO('yolov8n.pt')

# Open the video file
video_path = "/content/drive/MyDrive/Computer Vision/startic 3lvx enhanced.mp4"
cap = cv2.VideoCapture(video_path)

# Get the frame width and height
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

# Get the frame per second (fps)
fps = cap.get(cv2.CAP_PROP_FPS)

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
output_path = "/content/drive/MyDrive/Computer Vision/startic_annotated.mp4"
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Loop through the video frames
while cap.isOpened():
    # Read a frame from the video
    success, frame = cap.read()

    if success:
        # Run YOLOv8 inference on the frame
        results = model(frame)

        # Visualize the results on the frame
        annotated_frame = results[0].plot()

        # Convert the frame to BGR (OpenCV uses BGR)
        annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)

        # Write the annotated frame to the output video
        out.write(annotated_frame)

        # Display the annotated frame
        cv2_imshow(annotated_frame)

        # Break the loop if 'q' is pressed
        if cv2.waitKey(1) & 0xFF == ord("q"):
            break
    else:
        # Break the loop if the end of the video is reached
        break

# Release the video capture and writer objects
cap.release()
out.release()

from ultralytics import YOLO
model = YOLO('yolov8n.pt')


results = model(source = "/content/drive/MyDrive/Computer Vision/startic 3lvx enhanced.mp4", show=True,conf=0.4, save=True)

from ultralytics import YOLO
import cv2
import threading

# Load the YOLOv8 model
model = YOLO('yolov8n.pt')

# Function for running YOLO detection in the background
def detect_objects():
    global model, cap, out
    while cap.isOpened():
        success, frame = cap.read()
        if success:
            results = model(frame)
            annotated_frame = results[0].plot()
            annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_RGB2BGR)
            out.write(annotated_frame)
        else:
            break

# Open the video file
video_path = "/content/drive/MyDrive/Computer Vision/startic 3lvx enhanced.mp4"
cap = cv2.VideoCapture(video_path)

# Get the frame width, height, and frame rate
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
fps = cap.get(cv2.CAP_PROP_FPS)

# Define the codec and create VideoWriter object
fourcc = cv2.VideoWriter_fourcc(*'XVID')
output_path = "/content/drive/MyDrive/Computer Vision/startic_annotated.mp4"
out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))

# Start the thread for running YOLO detection in the background
detection_thread = threading.Thread(target=detect_objects)
detection_thread.start()

# Wait for the detection thread to finish
detection_thread.join()

# Release the video capture and writer objects
cap.release()
out.release()

OBJECT COUNTING

from ultralytics import YOLO
from ultralytics.solutions import object_counter
import cv2

model = YOLO("yolov8n.pt")
cap = cv2.VideoCapture("/content/drive/MyDrive/Computer Vision/startic 3lvx enhanced.mp4")
assert cap.isOpened(), "Error reading video file"
w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))

# Define region points
region_points = [(20, 400), (1080, 404), (1080, 360), (20, 360)]

# Video writer
video_writer = cv2.VideoWriter("object_counting_output.avi",
                       cv2.VideoWriter_fourcc(*'mp4v'),
                       fps,
                       (w, h))

# Init Object Counter
counter = object_counter.ObjectCounter()
counter.set_args(view_img=True,
                 reg_pts=region_points,
                 classes_names=model.names,
                 draw_tracks=True,
                 line_thickness=2)

while cap.isOpened():
    success, im0 = cap.read()
    if not success:
        print("Video frame is empty or video processing has been successfully completed.")
        break
    tracks = model.track(im0, persist=True, show=False)

    im0 = counter.start_counting(im0, tracks)
    video_writer.write(im0)

cap.release()
video_writer.release()
cv2.destroyAllWindows()
