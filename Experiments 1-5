{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21075,"status":"ok","timestamp":1752422336167,"user":{"displayName":"kemi phd","userId":"00656621715362041141"},"user_tz":-120},"id":"8upqpRVFZZPX","outputId":"eb61eb21-a8a4-47dd-8b82-09c19ab51839"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":112356,"status":"ok","timestamp":1752422451575,"user":{"displayName":"kemi phd","userId":"00656621715362041141"},"user_tz":-120},"id":"kkf_lFcSFeRM","outputId":"18f80a35-7e6f-48bb-d1a5-c574361d0d86"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.165-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.5)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.7.9)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.165-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.165 ultralytics-thop-2.0.14\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"X6QQgrsvEsKY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752422458457,"user_tz":-120,"elapsed":6885,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}},"outputId":"fc65c7fa-abd4-4dc0-ab4c-f4ac5b742267"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["# Ultralytics YOLO 🚀, AGPL-3.0 license\n","\n","import argparse\n","from collections import defaultdict\n","from pathlib import Path\n","\n","import cv2\n","import numpy as np\n","from shapely.geometry import Polygon\n","from shapely.geometry.point import Point\n","\n","from ultralytics import YOLO\n","from ultralytics.utils.files import increment_path\n","from ultralytics.utils.plotting import Annotator, colors\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"_Thq9IupE_95","executionInfo":{"status":"ok","timestamp":1752422458489,"user_tz":-120,"elapsed":17,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}}},"outputs":[],"source":["# Define global variables\n","track_history = defaultdict(list)\n","current_region = None\n","\n","counting_regions = [\n","    {\n","        \"name\": \"YOLOv8 Polygon Region\",\n","        \"polygon\": Polygon([(50, 80), (250, 20), (450, 80), (400, 350), (100, 350)]),\n","        \"counts\": 0,\n","        \"dragging\": False,\n","        \"region_color\": (255, 42, 4),\n","        \"text_color\": (255, 255, 255),\n","    },\n","    {\n","        \"name\": \"YOLOv8 Rectangle Region\",\n","        \"polygon\": Polygon([(200, 250), (440, 250), (440, 550), (200, 550)]),\n","        \"counts\": 0,\n","        \"dragging\": False,\n","        \"region_color\": (37, 255, 225),\n","        \"text_color\": (0, 0, 0),\n","    },\n","]\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"p1TDiES2FGdc","executionInfo":{"status":"ok","timestamp":1752422458518,"user_tz":-120,"elapsed":18,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}}},"outputs":[],"source":["# Define mouse callback function\n","def mouse_callback(event, x, y, flags, param):\n","    global current_region\n","\n","    if event == cv2.EVENT_LBUTTONDOWN:\n","        for region in counting_regions:\n","            if region[\"polygon\"].contains(Point((x, y))):\n","                current_region = region\n","                current_region[\"dragging\"] = True\n","                current_region[\"offset_x\"] = x\n","                current_region[\"offset_y\"] = y\n","\n","    elif event == cv2.EVENT_MOUSEMOVE:\n","        if current_region is not None and current_region[\"dragging\"]:\n","            dx = x - current_region[\"offset_x\"]\n","            dy = y - current_region[\"offset_y\"]\n","            current_region[\"polygon\"] = Polygon(\n","                [(p[0] + dx, p[1] + dy) for p in current_region[\"polygon\"].exterior.coords]\n","            )\n","            current_region[\"offset_x\"] = x\n","            current_region[\"offset_y\"] = y\n","\n","    elif event == cv2.EVENT_LBUTTONUP:\n","        if current_region is not None and current_region[\"dragging\"]:\n","            current_region[\"dragging\"] = False\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"fIzbZ84aFOJf","executionInfo":{"status":"ok","timestamp":1752422458620,"user_tz":-120,"elapsed":99,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}}},"outputs":[],"source":["# Define the main function\n","def run(weights=\"yolov8n.pt\", source=None, device=\"cpu\", view_img=False, save_img=True, exist_ok=False,\n","        classes=None, line_thickness=2, track_thickness=2, region_thickness=2):\n","\n","    vid_frame_count = 0\n","\n","    if not Path(source).exists():\n","        raise FileNotFoundError(f\"Source path '{source}' does not exist.\")\n","\n","    model = YOLO(f\"{weights}\")\n","    model.to(\"cuda\") if device == \"0\" else model.to(\"cpu\")\n","\n","    names = model.model.names\n","\n","    videocapture = cv2.VideoCapture(source)\n","    frame_width, frame_height = int(videocapture.get(3)), int(videocapture.get(4))\n","    fps, fourcc = int(videocapture.get(5)), cv2.VideoWriter_fourcc(*\"mp4v\")\n","\n","    save_dir = increment_path(Path(\"ultralytics_rc_output\") / \"exp\", exist_ok)\n","    save_dir.mkdir(parents=True, exist_ok=True)\n","    video_writer = cv2.VideoWriter(str(save_dir / f\"{Path(source).stem}.mp4\"), fourcc, fps, (frame_width, frame_height))\n","\n","    while videocapture.isOpened():\n","        success, frame = videocapture.read()\n","        if not success:\n","            break\n","        vid_frame_count += 1\n","\n","        results = model.track(frame, persist=True, classes=classes)\n","\n","        if results[0].boxes.id is not None:\n","            boxes = results[0].boxes.xyxy.cpu()\n","            track_ids = results[0].boxes.id.int().cpu().tolist()\n","            clss = results[0].boxes.cls.cpu().tolist()\n","\n","            annotator = Annotator(frame, line_width=line_thickness, example=str(names))\n","\n","            for box, track_id, cls in zip(boxes, track_ids, clss):\n","                annotator.box_label(box, str(names[cls]), color=colors(cls, True))\n","                bbox_center = (box[0] + box[2]) / 2, (box[1] + box[3]) / 2\n","\n","                track = track_history[track_id]\n","                track.append((float(bbox_center[0]), float(bbox_center[1])))\n","                if len(track) > 30:\n","                    track.pop(0)\n","                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n","                cv2.polylines(frame, [points], isClosed=False, color=colors(cls, True), thickness=track_thickness)\n","\n","                for region in counting_regions:\n","                    if region[\"polygon\"].contains(Point((bbox_center[0], bbox_center[1]))):\n","                        region[\"counts\"] += 1\n","\n","        for region in counting_regions:\n","            region_label = str(region[\"counts\"])\n","            region_color = region[\"region_color\"]\n","            region_text_color = region[\"text_color\"]\n","\n","            polygon_coords = np.array(region[\"polygon\"].exterior.coords, dtype=np.int32)\n","            centroid_x, centroid_y = int(region[\"polygon\"].centroid.x), int(region[\"polygon\"].centroid.y)\n","\n","            text_size, _ = cv2.getTextSize(\n","                region_label, cv2.FONT_HERSHEY_SIMPLEX, fontScale=0.7, thickness=line_thickness\n","            )\n","            text_x = centroid_x - text_size[0] // 2\n","            text_y = centroid_y + text_size[1] // 2\n","            cv2.rectangle(\n","                frame,\n","                (text_x - 5, text_y - text_size[1] - 5),\n","                (text_x + text_size[0] + 5, text_y + 5),\n","                region_color,\n","                -1,\n","            )\n","            cv2.putText(\n","                frame, region_label, (text_x, text_y), cv2.FONT_HERSHEY_SIMPLEX, 0.7, region_text_color, line_thickness\n","            )\n","            cv2.polylines(frame, [polygon_coords], isClosed=True, color=region_color, thickness=region_thickness)\n","\n","        if view_img:\n","            if vid_frame_count == 1:\n","                cv2.namedWindow(\"Ultralytics YOLOv8 Region Counter Movable\")\n","                cv2.setMouseCallback(\"Ultralytics YOLOv8 Region Counter Movable\", mouse_callback)\n","            cv2.imshow(\"Ultralytics YOLOv8 Region Counter Movable\", frame)\n","\n","        if save_img:\n","            video_writer.write(frame)\n","\n","        for region in counting_regions:\n","            region[\"counts\"] = 0\n","\n","        from IPython.display import display, clear_output\n","\n","\n","\n","\n","\n","    del vid_frame_count\n","    video_writer.release()\n","    videocapture.release()\n","    cv2.destroyAllWindows()\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"TeJ3NcflFP1G","executionInfo":{"status":"ok","timestamp":1752422458636,"user_tz":-120,"elapsed":3,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}}},"outputs":[],"source":["# Define argument parser function\n","def parse_opt():\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument(\"--weights\", type=str, default=\"yolov8n.pt\", help=\"initial weights path\")\n","    parser.add_argument(\"--device\", default=\"\", help=\"cuda device, i.e. 0 or 0,1,2,3 or cpu\")\n","    parser.add_argument(\"--source\", type=str, required=True, help=\"video file path\")\n","    parser.add_argument(\"--view-img\", action=\"store_true\", help=\"show results\")\n","    parser.add_argument(\"--save-img\", action=\"store_true\", help=\"save results\")\n","    parser.add_argument(\"--exist-ok\", action=\"store_true\", help=\"existing project/name ok, do not increment\")\n","    parser.add_argument(\"--classes\", nargs=\"+\", type=int, help=\"filter by class: --classes 0, or --classes 0 2 3\")\n","    parser.add_argument(\"--line-thickness\", type=int, default=2, help=\"bounding box thickness\")\n","    parser.add_argument(\"--track-thickness\", type=int, default=2, help=\"Tracking line thickness\")\n","    parser.add_argument(\"--region-thickness\", type=int, default=4, help=\"Region thickness\")\n","\n","    return parser.parse_args()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":115030,"status":"error","timestamp":1752415571570,"user":{"displayName":"kemi phd","userId":"00656621715362041141"},"user_tz":-120},"id":"ZM0uUB_JG9dG","outputId":"ba8546a1-f1e7-4ed7-bb13-d9101d059c5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.25M/6.25M [00:00<00:00, 60.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 1.2s\n","WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","0: 384x640 1 person, 2 cars, 1 traffic light, 526.1ms\n","Speed: 10.7ms preprocess, 526.1ms inference, 43.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 248.1ms\n","Speed: 4.2ms preprocess, 248.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 trucks, 229.7ms\n","Speed: 4.1ms preprocess, 229.7ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 272.4ms\n","Speed: 12.7ms preprocess, 272.4ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 253.3ms\n","Speed: 4.5ms preprocess, 253.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 239.1ms\n","Speed: 4.8ms preprocess, 239.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 221.5ms\n","Speed: 4.3ms preprocess, 221.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 167.8ms\n","Speed: 4.6ms preprocess, 167.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 157.6ms\n","Speed: 3.8ms preprocess, 157.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 144.0ms\n","Speed: 3.8ms preprocess, 144.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 143.4ms\n","Speed: 3.9ms preprocess, 143.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 traffic lights, 150.2ms\n","Speed: 5.5ms preprocess, 150.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 car, 2 traffic lights, 145.7ms\n","Speed: 4.3ms preprocess, 145.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 158.4ms\n","Speed: 4.6ms preprocess, 158.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 147.5ms\n","Speed: 3.8ms preprocess, 147.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 156.0ms\n","Speed: 5.3ms preprocess, 156.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 150.2ms\n","Speed: 3.7ms preprocess, 150.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 159.1ms\n","Speed: 4.2ms preprocess, 159.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 traffic lights, 174.8ms\n","Speed: 4.2ms preprocess, 174.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 1 handbag, 145.4ms\n","Speed: 5.7ms preprocess, 145.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 car, 1 bus, 1 truck, 1 traffic light, 143.5ms\n","Speed: 2.6ms preprocess, 143.5ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 138.8ms\n","Speed: 3.4ms preprocess, 138.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 146.1ms\n","Speed: 3.5ms preprocess, 146.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 154.2ms\n","Speed: 3.2ms preprocess, 154.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 142.1ms\n","Speed: 2.4ms preprocess, 142.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 135.8ms\n","Speed: 3.0ms preprocess, 135.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 135.3ms\n","Speed: 2.6ms preprocess, 135.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 136.9ms\n","Speed: 2.5ms preprocess, 136.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 car, 1 train, 140.2ms\n","Speed: 2.5ms preprocess, 140.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 car, 1 bus, 133.5ms\n","Speed: 2.3ms preprocess, 133.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 3 cars, 1 train, 1 truck, 2 traffic lights, 137.8ms\n","Speed: 2.7ms preprocess, 137.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 2 trains, 136.8ms\n","Speed: 4.8ms preprocess, 136.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 train, 1 truck, 2 traffic lights, 144.4ms\n","Speed: 5.5ms preprocess, 144.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 1 car, 1 train, 138.8ms\n","Speed: 4.5ms preprocess, 138.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 2 cars, 2 traffic lights, 1 potted plant, 161.3ms\n","Speed: 2.7ms preprocess, 161.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 22 persons, 1 train, 134.3ms\n","Speed: 2.5ms preprocess, 134.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 2 cars, 1 truck, 1 traffic light, 132.9ms\n","Speed: 2.5ms preprocess, 132.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 2 cars, 1 truck, 2 traffic lights, 137.1ms\n","Speed: 3.1ms preprocess, 137.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 137.9ms\n","Speed: 2.6ms preprocess, 137.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 159.7ms\n","Speed: 2.5ms preprocess, 159.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 138.5ms\n","Speed: 2.3ms preprocess, 138.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 145.6ms\n","Speed: 2.4ms preprocess, 145.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 136.1ms\n","Speed: 1.9ms preprocess, 136.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 142.5ms\n","Speed: 3.1ms preprocess, 142.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 167.9ms\n","Speed: 2.6ms preprocess, 167.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 143.5ms\n","Speed: 2.4ms preprocess, 143.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 3 cars, 1 bus, 1 truck, 1 traffic light, 148.6ms\n","Speed: 3.0ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 trains, 143.0ms\n","Speed: 2.4ms preprocess, 143.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 16 persons, 1 bus, 1 traffic light, 1 potted plant, 139.0ms\n","Speed: 2.5ms preprocess, 139.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 177.8ms\n","Speed: 2.5ms preprocess, 177.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bus, 1 traffic light, 140.7ms\n","Speed: 2.4ms preprocess, 140.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 car, 1 truck, 2 traffic lights, 140.4ms\n","Speed: 3.1ms preprocess, 140.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 1 bus, 1 train, 1 traffic light, 141.9ms\n","Speed: 3.0ms preprocess, 141.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 4 cars, 1 bus, 4 traffic lights, 135.0ms\n","Speed: 2.3ms preprocess, 135.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 3 cars, 1 bus, 1 traffic light, 142.4ms\n","Speed: 2.3ms preprocess, 142.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 2 cars, 1 bus, 1 traffic light, 156.6ms\n","Speed: 2.6ms preprocess, 156.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 cars, 2 traffic lights, 137.3ms\n","Speed: 2.2ms preprocess, 137.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 1 bus, 1 traffic light, 182.7ms\n","Speed: 3.4ms preprocess, 182.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 3 cars, 4 traffic lights, 209.0ms\n","Speed: 2.9ms preprocess, 209.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 225.4ms\n","Speed: 2.6ms preprocess, 225.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 220.5ms\n","Speed: 2.5ms preprocess, 220.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 traffic lights, 204.5ms\n","Speed: 2.6ms preprocess, 204.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 3 horses, 1 potted plant, 233.5ms\n","Speed: 2.7ms preprocess, 233.5ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 car, 1 train, 2 traffic lights, 1 potted plant, 216.1ms\n","Speed: 3.0ms preprocess, 216.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 car, 2 traffic lights, 207.8ms\n","Speed: 2.7ms preprocess, 207.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 1 traffic light, 207.2ms\n","Speed: 2.5ms preprocess, 207.2ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 2 cars, 223.1ms\n","Speed: 2.5ms preprocess, 223.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 208.6ms\n","Speed: 3.1ms preprocess, 208.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 2 traffic lights, 214.7ms\n","Speed: 3.5ms preprocess, 214.7ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 223.0ms\n","Speed: 3.4ms preprocess, 223.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 2 cars, 3 traffic lights, 241.0ms\n","Speed: 2.3ms preprocess, 241.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 traffic light, 193.3ms\n","Speed: 2.4ms preprocess, 193.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 traffic light, 136.0ms\n","Speed: 2.3ms preprocess, 136.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 137.0ms\n","Speed: 2.6ms preprocess, 137.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 141.0ms\n","Speed: 3.6ms preprocess, 141.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 149.8ms\n","Speed: 2.7ms preprocess, 149.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 135.0ms\n","Speed: 2.3ms preprocess, 135.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 135.4ms\n","Speed: 3.2ms preprocess, 135.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 138.9ms\n","Speed: 3.5ms preprocess, 138.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 137.8ms\n","Speed: 2.4ms preprocess, 137.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 140.7ms\n","Speed: 2.4ms preprocess, 140.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 149.2ms\n","Speed: 2.6ms preprocess, 149.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 134.0ms\n","Speed: 2.8ms preprocess, 134.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 138.5ms\n","Speed: 2.8ms preprocess, 138.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 142.4ms\n","Speed: 2.9ms preprocess, 142.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 138.1ms\n","Speed: 2.7ms preprocess, 138.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 147.2ms\n","Speed: 2.4ms preprocess, 147.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 135.3ms\n","Speed: 3.0ms preprocess, 135.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 134.9ms\n","Speed: 2.6ms preprocess, 134.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 147.7ms\n","Speed: 2.6ms preprocess, 147.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 142.5ms\n","Speed: 2.4ms preprocess, 142.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 147.6ms\n","Speed: 2.5ms preprocess, 147.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 177.3ms\n","Speed: 3.3ms preprocess, 177.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 148.8ms\n","Speed: 2.9ms preprocess, 148.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 154.1ms\n","Speed: 2.8ms preprocess, 154.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 139.7ms\n","Speed: 2.2ms preprocess, 139.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 136.2ms\n","Speed: 3.0ms preprocess, 136.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 151.7ms\n","Speed: 3.3ms preprocess, 151.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 bus, 1 truck, 156.3ms\n","Speed: 3.0ms preprocess, 156.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 buss, 143.4ms\n","Speed: 2.3ms preprocess, 143.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 buss, 141.1ms\n","Speed: 2.4ms preprocess, 141.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 buss, 143.2ms\n","Speed: 2.5ms preprocess, 143.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 buss, 159.4ms\n","Speed: 3.8ms preprocess, 159.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 buss, 135.3ms\n","Speed: 2.6ms preprocess, 135.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 buss, 143.2ms\n","Speed: 2.9ms preprocess, 143.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 buss, 140.6ms\n","Speed: 2.4ms preprocess, 140.6ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 1 bus, 1 train, 135.1ms\n","Speed: 2.3ms preprocess, 135.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 buss, 134.1ms\n","Speed: 2.3ms preprocess, 134.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 1 train, 150.2ms\n","Speed: 3.1ms preprocess, 150.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 train, 149.7ms\n","Speed: 3.1ms preprocess, 149.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 truck, 152.6ms\n","Speed: 3.0ms preprocess, 152.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 traffic light, 145.8ms\n","Speed: 2.5ms preprocess, 145.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 traffic lights, 134.9ms\n","Speed: 3.1ms preprocess, 134.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 149.6ms\n","Speed: 2.8ms preprocess, 149.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 2 traffic lights, 137.9ms\n","Speed: 2.5ms preprocess, 137.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 1 traffic light, 139.4ms\n","Speed: 2.9ms preprocess, 139.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 1 traffic light, 141.0ms\n","Speed: 3.5ms preprocess, 141.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 train, 1 traffic light, 139.6ms\n","Speed: 3.5ms preprocess, 139.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 1 traffic light, 134.9ms\n","Speed: 3.4ms preprocess, 134.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 141.4ms\n","Speed: 2.9ms preprocess, 141.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 142.6ms\n","Speed: 2.4ms preprocess, 142.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 139.6ms\n","Speed: 3.0ms preprocess, 139.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 135.4ms\n","Speed: 2.8ms preprocess, 135.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 140.8ms\n","Speed: 2.9ms preprocess, 140.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 traffic light, 229.3ms\n","Speed: 2.7ms preprocess, 229.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 232.3ms\n","Speed: 2.7ms preprocess, 232.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 bus, 210.2ms\n","Speed: 2.3ms preprocess, 210.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 210.5ms\n","Speed: 2.4ms preprocess, 210.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 1 traffic light, 209.9ms\n","Speed: 3.8ms preprocess, 209.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 traffic light, 214.2ms\n","Speed: 2.5ms preprocess, 214.2ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 traffic light, 212.4ms\n","Speed: 3.2ms preprocess, 212.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 211.5ms\n","Speed: 2.3ms preprocess, 211.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 traffic lights, 212.3ms\n","Speed: 2.4ms preprocess, 212.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 203.4ms\n","Speed: 2.5ms preprocess, 203.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 200.9ms\n","Speed: 2.6ms preprocess, 200.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 231.5ms\n","Speed: 2.8ms preprocess, 231.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 221.0ms\n","Speed: 3.5ms preprocess, 221.0ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 trucks, 1 traffic light, 232.2ms\n","Speed: 2.5ms preprocess, 232.2ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 1 truck, 1 traffic light, 232.0ms\n","Speed: 3.6ms preprocess, 232.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 1 truck, 195.8ms\n","Speed: 2.4ms preprocess, 195.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 buss, 164.7ms\n","Speed: 2.8ms preprocess, 164.7ms inference, 6.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 bus, 1 truck, 156.9ms\n","Speed: 2.7ms preprocess, 156.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 bus, 1 truck, 136.8ms\n","Speed: 2.5ms preprocess, 136.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 3 trucks, 160.6ms\n","Speed: 2.2ms preprocess, 160.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 bus, 166.9ms\n","Speed: 2.7ms preprocess, 166.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 1 bus, 191.5ms\n","Speed: 2.9ms preprocess, 191.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 4 trucks, 146.6ms\n","Speed: 2.9ms preprocess, 146.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 cars, 1 bus, 2 trucks, 145.0ms\n","Speed: 3.0ms preprocess, 145.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 cars, 1 bus, 168.5ms\n","Speed: 3.1ms preprocess, 168.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 cars, 1 bus, 1 truck, 139.8ms\n","Speed: 2.4ms preprocess, 139.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 cars, 1 truck, 148.6ms\n","Speed: 3.6ms preprocess, 148.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 137.6ms\n","Speed: 2.6ms preprocess, 137.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 157.6ms\n","Speed: 3.4ms preprocess, 157.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 truck, 164.0ms\n","Speed: 3.4ms preprocess, 164.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 134.9ms\n","Speed: 2.2ms preprocess, 134.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 135.0ms\n","Speed: 2.5ms preprocess, 135.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 132.0ms\n","Speed: 2.5ms preprocess, 132.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 219.1ms\n","Speed: 2.5ms preprocess, 219.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 153.6ms\n","Speed: 3.0ms preprocess, 153.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 car, 1 bus, 2 cell phones, 135.0ms\n","Speed: 2.9ms preprocess, 135.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 3 cars, 1 bus, 2 trucks, 4 traffic lights, 137.7ms\n","Speed: 2.2ms preprocess, 137.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 2 cars, 1 traffic light, 2 cell phones, 173.1ms\n","Speed: 2.5ms preprocess, 173.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 cars, 1 bus, 1 truck, 5 traffic lights, 3 cell phones, 145.9ms\n","Speed: 2.8ms preprocess, 145.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 truck, 1 traffic light, 3 cell phones, 135.0ms\n","Speed: 2.5ms preprocess, 135.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 2 cars, 4 trucks, 1 traffic light, 139.8ms\n","Speed: 2.8ms preprocess, 139.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 137.0ms\n","Speed: 2.4ms preprocess, 137.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 12 traffic lights, 136.2ms\n","Speed: 3.2ms preprocess, 136.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 traffic light, 137.1ms\n","Speed: 2.4ms preprocess, 137.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 cars, 1 bus, 3 trucks, 5 traffic lights, 138.9ms\n","Speed: 2.6ms preprocess, 138.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 156.8ms\n","Speed: 2.5ms preprocess, 156.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 6 cars, 1 truck, 4 traffic lights, 139.3ms\n","Speed: 2.5ms preprocess, 139.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 cars, 2 trucks, 5 traffic lights, 1 cell phone, 135.5ms\n","Speed: 2.5ms preprocess, 135.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 2 trucks, 2 traffic lights, 144.2ms\n","Speed: 2.6ms preprocess, 144.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 140.6ms\n","Speed: 2.4ms preprocess, 140.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 traffic light, 139.8ms\n","Speed: 3.1ms preprocess, 139.8ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 parking meter, 136.3ms\n","Speed: 2.5ms preprocess, 136.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 traffic light, 135.5ms\n","Speed: 2.5ms preprocess, 135.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 138.5ms\n","Speed: 3.3ms preprocess, 138.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 truck, 146.7ms\n","Speed: 3.0ms preprocess, 146.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 133.4ms\n","Speed: 3.0ms preprocess, 133.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 153.5ms\n","Speed: 2.5ms preprocess, 153.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 133.2ms\n","Speed: 2.4ms preprocess, 133.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 137.7ms\n","Speed: 2.7ms preprocess, 137.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 2 cars, 1 traffic light, 134.7ms\n","Speed: 2.3ms preprocess, 134.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 traffic light, 1 fire hydrant, 131.5ms\n","Speed: 2.9ms preprocess, 131.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 1 fire hydrant, 148.3ms\n","Speed: 2.3ms preprocess, 148.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 134.9ms\n","Speed: 3.1ms preprocess, 134.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 138.1ms\n","Speed: 2.6ms preprocess, 138.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 141.4ms\n","Speed: 2.4ms preprocess, 141.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 225.7ms\n","Speed: 2.5ms preprocess, 225.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 232.5ms\n","Speed: 2.5ms preprocess, 232.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 215.4ms\n","Speed: 2.3ms preprocess, 215.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 213.4ms\n","Speed: 3.1ms preprocess, 213.4ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 217.1ms\n","Speed: 2.8ms preprocess, 217.1ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 213.9ms\n","Speed: 4.7ms preprocess, 213.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 215.4ms\n","Speed: 4.6ms preprocess, 215.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 221.6ms\n","Speed: 2.5ms preprocess, 221.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 216.8ms\n","Speed: 2.3ms preprocess, 216.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 230.1ms\n","Speed: 3.2ms preprocess, 230.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 210.2ms\n","Speed: 2.2ms preprocess, 210.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 220.3ms\n","Speed: 3.2ms preprocess, 220.3ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 228.8ms\n","Speed: 2.6ms preprocess, 228.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 215.5ms\n","Speed: 2.5ms preprocess, 215.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 152.9ms\n","Speed: 2.5ms preprocess, 152.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 141.3ms\n","Speed: 2.8ms preprocess, 141.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 1 truck, 140.1ms\n","Speed: 2.0ms preprocess, 140.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 trucks, 136.3ms\n","Speed: 2.5ms preprocess, 136.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 trucks, 135.9ms\n","Speed: 3.2ms preprocess, 135.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 trucks, 135.9ms\n","Speed: 2.6ms preprocess, 135.9ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 truck, 132.2ms\n","Speed: 2.4ms preprocess, 132.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 149.3ms\n","Speed: 2.9ms preprocess, 149.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 car, 143.5ms\n","Speed: 2.9ms preprocess, 143.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 141.3ms\n","Speed: 3.7ms preprocess, 141.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 138.5ms\n","Speed: 2.9ms preprocess, 138.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 140.7ms\n","Speed: 2.4ms preprocess, 140.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 2 cars, 1 bus, 4 traffic lights, 150.3ms\n","Speed: 2.6ms preprocess, 150.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 7 cars, 2 trucks, 134.8ms\n","Speed: 2.6ms preprocess, 134.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 7 cars, 1 bus, 1 truck, 1 cell phone, 133.1ms\n","Speed: 3.2ms preprocess, 133.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 8 cars, 1 bus, 1 truck, 1 traffic light, 1 horse, 1 cell phone, 136.5ms\n","Speed: 2.7ms preprocess, 136.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 8 persons, 2 cars, 1 airplane, 2 trucks, 1 cell phone, 136.9ms\n","Speed: 2.5ms preprocess, 136.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 2 cars, 1 motorcycle, 1 bus, 1 dog, 1 horse, 1 cell phone, 147.5ms\n","Speed: 3.3ms preprocess, 147.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 2 cars, 2 buss, 1 cell phone, 144.6ms\n","Speed: 2.4ms preprocess, 144.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 cell phone, 1 oven, 135.9ms\n","Speed: 3.4ms preprocess, 135.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 137.4ms\n","Speed: 3.7ms preprocess, 137.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 135.7ms\n","Speed: 3.5ms preprocess, 135.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 138.0ms\n","Speed: 5.2ms preprocess, 138.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 156.9ms\n","Speed: 2.7ms preprocess, 156.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 140.8ms\n","Speed: 2.6ms preprocess, 140.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 145.9ms\n","Speed: 2.6ms preprocess, 145.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 140.0ms\n","Speed: 2.3ms preprocess, 140.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 154.7ms\n","Speed: 3.3ms preprocess, 154.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 173.7ms\n","Speed: 2.6ms preprocess, 173.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 155.1ms\n","Speed: 2.2ms preprocess, 155.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 154.2ms\n","Speed: 4.3ms preprocess, 154.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 train, 1 dog, 167.8ms\n","Speed: 2.6ms preprocess, 167.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 bus, 1 truck, 1 traffic light, 140.4ms\n","Speed: 2.5ms preprocess, 140.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 train, 1 traffic light, 169.1ms\n","Speed: 3.5ms preprocess, 169.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 truck, 1 traffic light, 146.7ms\n","Speed: 2.5ms preprocess, 146.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 1 traffic light, 2 dogs, 148.8ms\n","Speed: 2.6ms preprocess, 148.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 149.1ms\n","Speed: 2.7ms preprocess, 149.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 144.1ms\n","Speed: 3.0ms preprocess, 144.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 car, 2 trucks, 165.7ms\n","Speed: 2.5ms preprocess, 165.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 2 cars, 1 bus, 1 truck, 1 traffic light, 145.1ms\n","Speed: 2.6ms preprocess, 145.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 152.2ms\n","Speed: 2.6ms preprocess, 152.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 car, 2 trains, 2 traffic lights, 143.1ms\n","Speed: 2.9ms preprocess, 143.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 1 car, 1 train, 1 dog, 153.0ms\n","Speed: 3.3ms preprocess, 153.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 2 buss, 2 trucks, 156.6ms\n","Speed: 2.4ms preprocess, 156.6ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 2 cars, 1 bus, 1 truck, 1 potted plant, 147.4ms\n","Speed: 2.8ms preprocess, 147.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 cars, 2 trucks, 147.0ms\n","Speed: 2.8ms preprocess, 147.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 2 trucks, 1 traffic light, 146.5ms\n","Speed: 3.1ms preprocess, 146.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 2 cars, 1 truck, 1 traffic light, 147.3ms\n","Speed: 2.8ms preprocess, 147.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 148.1ms\n","Speed: 2.2ms preprocess, 148.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 2 cars, 1 truck, 1 traffic light, 141.3ms\n","Speed: 2.3ms preprocess, 141.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 1 car, 2 trucks, 2 traffic lights, 191.4ms\n","Speed: 2.6ms preprocess, 191.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 3 cars, 1 bus, 225.7ms\n","Speed: 2.5ms preprocess, 225.7ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 236.2ms\n","Speed: 3.0ms preprocess, 236.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 2 cars, 1 bus, 1 traffic light, 217.3ms\n","Speed: 2.5ms preprocess, 217.3ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 car, 3 trucks, 2 traffic lights, 229.7ms\n","Speed: 2.6ms preprocess, 229.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 3 trucks, 1 traffic light, 235.3ms\n","Speed: 2.6ms preprocess, 235.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 2 trucks, 229.6ms\n","Speed: 2.4ms preprocess, 229.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 230.1ms\n","Speed: 2.8ms preprocess, 230.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 227.1ms\n","Speed: 3.5ms preprocess, 227.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 242.5ms\n","Speed: 2.4ms preprocess, 242.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 541.5ms\n","Speed: 2.5ms preprocess, 541.5ms inference, 12.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 789.2ms\n","Speed: 13.6ms preprocess, 789.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 150.1ms\n","Speed: 2.3ms preprocess, 150.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 575.6ms\n","Speed: 3.2ms preprocess, 575.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 283.9ms\n","Speed: 2.5ms preprocess, 283.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 180.5ms\n","Speed: 2.7ms preprocess, 180.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 6 persons, 4 cars, 1 truck, 2 traffic lights, 378.1ms\n","Speed: 2.0ms preprocess, 378.1ms inference, 12.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 288.8ms\n","Speed: 2.6ms preprocess, 288.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 142.7ms\n","Speed: 2.9ms preprocess, 142.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 149.2ms\n","Speed: 3.0ms preprocess, 149.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 140.9ms\n","Speed: 2.8ms preprocess, 140.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 144.3ms\n","Speed: 3.8ms preprocess, 144.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 156.8ms\n","Speed: 3.5ms preprocess, 156.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 151.5ms\n","Speed: 3.9ms preprocess, 151.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 146.8ms\n","Speed: 3.1ms preprocess, 146.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 144.2ms\n","Speed: 2.3ms preprocess, 144.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 140.3ms\n","Speed: 3.3ms preprocess, 140.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 15 persons, 2 cars, 1 bus, 3 trucks, 152.0ms\n","Speed: 2.8ms preprocess, 152.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 cars, 146.3ms\n","Speed: 2.5ms preprocess, 146.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 3 cars, 2 trucks, 142.9ms\n","Speed: 2.8ms preprocess, 142.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 3 cars, 2 buss, 1 traffic light, 153.5ms\n","Speed: 2.6ms preprocess, 153.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 car, 2 trucks, 146.9ms\n","Speed: 2.4ms preprocess, 146.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 5 traffic lights, 144.1ms\n","Speed: 3.1ms preprocess, 144.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 3 cars, 1 bus, 146.8ms\n","Speed: 2.3ms preprocess, 146.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 1 bus, 142.1ms\n","Speed: 2.3ms preprocess, 142.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 car, 3 traffic lights, 146.7ms\n","Speed: 3.0ms preprocess, 146.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 179.9ms\n","Speed: 3.6ms preprocess, 179.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 2 buss, 172.8ms\n","Speed: 2.9ms preprocess, 172.8ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 151.7ms\n","Speed: 3.4ms preprocess, 151.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 146.2ms\n","Speed: 2.3ms preprocess, 146.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 142.7ms\n","Speed: 2.7ms preprocess, 142.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 2 cars, 1 train, 1 truck, 3 traffic lights, 1 tv, 145.6ms\n","Speed: 2.5ms preprocess, 145.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 142.3ms\n","Speed: 2.3ms preprocess, 142.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 4 cars, 2 buss, 155.7ms\n","Speed: 2.3ms preprocess, 155.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 139.0ms\n","Speed: 2.6ms preprocess, 139.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 138.0ms\n","Speed: 2.5ms preprocess, 138.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 142.8ms\n","Speed: 2.3ms preprocess, 142.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 148.4ms\n","Speed: 2.3ms preprocess, 148.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 172.9ms\n","Speed: 4.0ms preprocess, 172.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 143.2ms\n","Speed: 2.6ms preprocess, 143.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 142.9ms\n","Speed: 2.6ms preprocess, 142.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 141.6ms\n","Speed: 2.5ms preprocess, 141.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 156.2ms\n","Speed: 2.5ms preprocess, 156.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 163.8ms\n","Speed: 2.8ms preprocess, 163.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 7 persons, 2 cars, 1 bus, 1 train, 198.0ms\n","Speed: 2.5ms preprocess, 198.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 222.0ms\n","Speed: 2.3ms preprocess, 222.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 220.1ms\n","Speed: 4.1ms preprocess, 220.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 10 persons, 2 cars, 1 train, 1 truck, 3 traffic lights, 258.0ms\n","Speed: 2.5ms preprocess, 258.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 trains, 1 truck, 214.0ms\n","Speed: 2.6ms preprocess, 214.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 238.8ms\n","Speed: 2.7ms preprocess, 238.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 train, 236.8ms\n","Speed: 4.0ms preprocess, 236.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 1 bus, 217.8ms\n","Speed: 7.5ms preprocess, 217.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 216.1ms\n","Speed: 5.5ms preprocess, 216.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 223.2ms\n","Speed: 4.6ms preprocess, 223.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 1 traffic light, 218.1ms\n","Speed: 3.9ms preprocess, 218.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 1 traffic light, 223.8ms\n","Speed: 3.1ms preprocess, 223.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 243.0ms\n","Speed: 3.2ms preprocess, 243.0ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 1 traffic light, 239.2ms\n","Speed: 2.7ms preprocess, 239.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 1 car, 2 traffic lights, 216.3ms\n","Speed: 3.7ms preprocess, 216.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 155.4ms\n","Speed: 3.2ms preprocess, 155.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 159.6ms\n","Speed: 2.6ms preprocess, 159.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 147.8ms\n","Speed: 3.5ms preprocess, 147.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 169.8ms\n","Speed: 2.8ms preprocess, 169.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 155.5ms\n","Speed: 2.9ms preprocess, 155.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 11 persons, 3 cars, 3 trucks, 1 traffic light, 162.1ms\n","Speed: 2.7ms preprocess, 162.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 12 persons, 2 cars, 1 train, 1 truck, 157.8ms\n","Speed: 4.3ms preprocess, 157.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 143.0ms\n","Speed: 3.0ms preprocess, 143.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 155.6ms\n","Speed: 3.9ms preprocess, 155.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 144.9ms\n","Speed: 2.4ms preprocess, 144.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 142.5ms\n","Speed: 3.3ms preprocess, 142.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 148.6ms\n","Speed: 3.1ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 138.7ms\n","Speed: 3.2ms preprocess, 138.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 160.7ms\n","Speed: 2.9ms preprocess, 160.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 train, 140.6ms\n","Speed: 2.5ms preprocess, 140.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 140.0ms\n","Speed: 2.6ms preprocess, 140.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 146.8ms\n","Speed: 2.5ms preprocess, 146.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 train, 138.9ms\n","Speed: 2.4ms preprocess, 138.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 141.6ms\n","Speed: 2.5ms preprocess, 141.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 141.7ms\n","Speed: 2.2ms preprocess, 141.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 146.8ms\n","Speed: 2.4ms preprocess, 146.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 145.6ms\n","Speed: 4.5ms preprocess, 145.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 cell phone, 144.0ms\n","Speed: 2.3ms preprocess, 144.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 cell phone, 139.2ms\n","Speed: 2.6ms preprocess, 139.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 142.5ms\n","Speed: 2.4ms preprocess, 142.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 144.1ms\n","Speed: 2.4ms preprocess, 144.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 cell phone, 148.6ms\n","Speed: 2.9ms preprocess, 148.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 146.2ms\n","Speed: 2.6ms preprocess, 146.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 141.5ms\n","Speed: 3.2ms preprocess, 141.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 158.1ms\n","Speed: 2.2ms preprocess, 158.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 cell phone, 149.4ms\n","Speed: 2.3ms preprocess, 149.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 3 cars, 3 trucks, 148.8ms\n","Speed: 2.3ms preprocess, 148.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 bus, 3 trucks, 1 traffic light, 1 cell phone, 148.5ms\n","Speed: 3.6ms preprocess, 148.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 1 bus, 3 trucks, 4 traffic lights, 1 cell phone, 149.8ms\n","Speed: 2.3ms preprocess, 149.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 3 cars, 1 truck, 1 traffic light, 4 cell phones, 196.0ms\n","Speed: 2.6ms preprocess, 196.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 1 car, 1 truck, 1 traffic light, 1 bottle, 2 cell phones, 1 book, 152.3ms\n","Speed: 2.5ms preprocess, 152.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 persons, 3 cars, 2 trucks, 1 traffic light, 150.1ms\n","Speed: 2.4ms preprocess, 150.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 5 cars, 1 truck, 144.3ms\n","Speed: 3.5ms preprocess, 144.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 persons, 2 cars, 3 buss, 4 trucks, 1 cell phone, 142.0ms\n","Speed: 3.0ms preprocess, 142.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 13 persons, 1 car, 1 bus, 4 trucks, 1 traffic light, 143.6ms\n","Speed: 4.2ms preprocess, 143.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 149.1ms\n","Speed: 2.6ms preprocess, 149.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 151.5ms\n","Speed: 3.0ms preprocess, 151.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 2 trucks, 145.6ms\n","Speed: 2.5ms preprocess, 145.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 4 cars, 145.6ms\n","Speed: 2.5ms preprocess, 145.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 truck, 158.9ms\n","Speed: 2.4ms preprocess, 158.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 3 cars, 162.4ms\n","Speed: 4.3ms preprocess, 162.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 141.5ms\n","Speed: 2.4ms preprocess, 141.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 139.3ms\n","Speed: 3.1ms preprocess, 139.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 258.4ms\n","Speed: 2.8ms preprocess, 258.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 2 trucks, 225.9ms\n","Speed: 2.7ms preprocess, 225.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 1 truck, 227.4ms\n","Speed: 6.9ms preprocess, 227.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 truck, 220.1ms\n","Speed: 3.8ms preprocess, 220.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 219.8ms\n","Speed: 3.6ms preprocess, 219.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 cars, 221.9ms\n","Speed: 3.3ms preprocess, 221.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 219.4ms\n","Speed: 2.4ms preprocess, 219.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 238.8ms\n","Speed: 2.5ms preprocess, 238.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 215.6ms\n","Speed: 2.5ms preprocess, 215.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 210.0ms\n","Speed: 2.4ms preprocess, 210.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 229.6ms\n","Speed: 6.7ms preprocess, 229.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 235.8ms\n","Speed: 2.5ms preprocess, 235.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 223.2ms\n","Speed: 2.4ms preprocess, 223.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 204.3ms\n","Speed: 2.3ms preprocess, 204.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 147.4ms\n","Speed: 2.8ms preprocess, 147.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 148.6ms\n","Speed: 3.3ms preprocess, 148.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 149.0ms\n","Speed: 2.4ms preprocess, 149.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 truck, 149.7ms\n","Speed: 2.6ms preprocess, 149.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 143.7ms\n","Speed: 5.3ms preprocess, 143.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 140.5ms\n","Speed: 2.3ms preprocess, 140.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 155.5ms\n","Speed: 2.4ms preprocess, 155.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 141.3ms\n","Speed: 2.3ms preprocess, 141.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 146.8ms\n","Speed: 2.8ms preprocess, 146.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 car, 1 bus, 149.5ms\n","Speed: 2.2ms preprocess, 149.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 2 persons, 4 cars, 1 bus, 1 truck, 2 cell phones, 150.3ms\n","Speed: 2.4ms preprocess, 150.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 4 cars, 1 truck, 153.4ms\n","Speed: 2.2ms preprocess, 153.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 3 cars, 1 truck, 1 tv, 2 cell phones, 153.8ms\n","Speed: 2.5ms preprocess, 153.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 9 persons, 1 car, 1 truck, 2 cell phones, 159.6ms\n","Speed: 2.6ms preprocess, 159.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 bus, 1 traffic light, 1 cell phone, 161.0ms\n","Speed: 2.4ms preprocess, 161.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 1 person, 150.3ms\n","Speed: 2.5ms preprocess, 150.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n","\n","0: 384x640 5 persons, 1 bus, 154.3ms\n","Speed: 2.6ms preprocess, 154.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n"]},{"output_type":"error","ename":"error","evalue":"OpenCV(4.12.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-9-3288973722.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_videoAnnotated.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-7-3484352366.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(weights, source, device, view_img, save_img, exist_ok, classes, line_thickness, track_thickness, region_thickness)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0mvideo_writer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0mvideocapture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroyAllWindows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31merror\u001b[0m: OpenCV(4.12.0) /io/opencv/modules/highgui/src/window.cpp:1295: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvDestroyAllWindows'\n"]}],"source":["run(source=\"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_videoAnnotated.mp4\")"]},{"cell_type":"markdown","metadata":{"id":"FzWCWhjUYd0D"},"source":["Real Time Counting Metrics Result"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2054,"status":"ok","timestamp":1752415593942,"user":{"displayName":"kemi phd","userId":"00656621715362041141"},"user_tz":-120},"id":"fG14W5dYHVeF","outputId":"4c9756c3-4230-4d20-f5ba-a1a2cc5007dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Object Counting Metrics:\n","Accuracy (%): 42.86\n","Precision (%): 100.0\n","Recall (%): 100.0\n","F1-Score (%): 100.0\n","MAE: 0.57\n"]}],"source":["import numpy as np\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n","\n","def evaluate_metrics(gt_counts, pred_counts):\n","    \"\"\"\n","    Evaluate object counting performance.\n","\n","    Args:\n","    - gt_counts: list of ground truth object counts per frame\n","    - pred_counts: list of predicted object counts per frame\n","\n","    Returns:\n","    - dict of evaluation metrics\n","    \"\"\"\n","    # Convert to numpy arrays\n","    gt_counts = np.array(gt_counts)\n","    pred_counts = np.array(pred_counts)\n","\n","    # For classification metrics, define True Positives (TP) as correct count\n","    # We'll treat each frame as a separate sample\n","    correct = gt_counts == pred_counts\n","    accuracy = np.mean(correct) * 100\n","\n","    # MAE\n","    mae = np.mean(np.abs(gt_counts - pred_counts))\n","\n","    # Optional binarization for classification metrics (use threshold or exact match)\n","    binary_gt = (gt_counts > 0).astype(int)\n","    binary_pred = (pred_counts > 0).astype(int)\n","\n","    precision = precision_score(binary_gt, binary_pred, zero_division=0) * 100\n","    recall = recall_score(binary_gt, binary_pred, zero_division=0) * 100\n","    f1 = f1_score(binary_gt, binary_pred, zero_division=0) * 100\n","\n","    return {\n","        \"Accuracy (%)\": round(accuracy, 2),\n","        \"Precision (%)\": round(precision, 2),\n","        \"Recall (%)\": round(recall, 2),\n","        \"F1-Score (%)\": round(f1, 2),\n","        \"MAE\": round(mae, 2)\n","    }\n","\n","# Example ground truth and predicted counts\n","gt_counts = [2, 3, 4, 2, 0, 1, 3]       # You should replace this with your actual data\n","pred_counts = [2, 3, 5, 1, 0, 2, 2]     # Model's predicted object count per frame\n","\n","metrics = evaluate_metrics(gt_counts, pred_counts)\n","print(\"Object Counting Metrics:\")\n","for k, v in metrics.items():\n","    print(f\"{k}: {v}\")"]},{"cell_type":"markdown","metadata":{"id":"Hne3p6VgM3y6"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q0cav7YyNbiX"},"outputs":[],"source":["def apply_threshold(img, method='fixed', **kwargs):\n","    if method == 'fixed':\n","        _, thresh = cv2.threshold(img, kwargs.get('thresh', 127), 255, cv2.THRESH_BINARY)\n","    elif method == 'otsu':\n","        _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","    elif method == 'niblack':\n","        from skimage.filters import threshold_niblack\n","        window_size = kwargs.get('window_size', 25)\n","        k = kwargs.get('k', 0.8)\n","        niblack_thresh = threshold_niblack(img, window_size=window_size, k=k)\n","        thresh = img > niblack_thresh\n","    else:\n","        raise ValueError(\"Unknown method\")\n","\n","    return thresh.astype(np.uint8) * 255 if method != 'niblack' else thresh"]},{"cell_type":"markdown","metadata":{"id":"EDEoM0W-GUPh"},"source":["Experiment Two"]},{"cell_type":"markdown","metadata":{"id":"Z2Tu9xZFDqe6"},"source":["Single Threading Implementation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":205756,"status":"ok","timestamp":1752415839115,"user":{"displayName":"kemi phd","userId":"00656621715362041141"},"user_tz":-120},"id":"r_Nl2kjsDoJj","outputId":"2a8c1174-0e5c-46ba-d712-1bf97a58aa2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running inference on: /content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video2.mp4\n","\n","=== Single Thread Performance (YOLOv8) ===\n","Total Frames Processed: 714\n","Average FPS: 3.59\n","Preprocessing Latency: 1.36 ms\n","Inference Latency: 276.21 ms\n","Postprocessing Latency: 0.02 ms\n","Total Frame Latency: 277.59 ms\n","CPU Utilization: 55.60%\n","GPU Utilization: 0.00%\n"]},{"output_type":"execute_result","data":{"text/plain":["{'fps': 3.5908954936697888,\n"," 'pre': 1.3580078504332642,\n"," 'infer': 276.213499010444,\n"," 'post': 0.021962892441522507,\n"," 'total': 277.5934697533188,\n"," 'cpu': 55.6,\n"," 'gpu': 0}"]},"metadata":{},"execution_count":11}],"source":["# ✅ Install YOLOv8 if not already installed\n","!pip install -q ultralytics psutil opencv-python\n","\n","import cv2\n","import time\n","import psutil\n","import torch\n","import numpy as np\n","from ultralytics import YOLO\n","\n","# ✅ Preprocess / Inference / Postprocess per frame\n","def process_frame_single(model, frame):\n","    t1 = time.time()\n","    img = cv2.resize(frame, (640, 640))  # Resize to YOLOv8's default input\n","    t2 = time.time()\n","\n","    # Inference using Ultralytics YOLOv8 API\n","    results = model.predict(img, verbose=False)\n","    t3 = time.time()\n","\n","    # Postprocessing: Count detections\n","    boxes = results[0].boxes\n","    count = len(boxes) if boxes is not None else 0\n","    t4 = time.time()\n","\n","    return {\n","        \"annotated\": img,\n","        \"pre_time\": (t2 - t1) * 1000,\n","        \"infer_time\": (t3 - t2) * 1000,\n","        \"post_time\": (t4 - t3) * 1000,\n","        \"count\": count\n","    }\n","\n","# ✅ Single-threaded runner\n","def run_single_thread(video_path, model):\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","    total_pre, total_infer, total_post, total_latency = 0, 0, 0, 0\n","    start_time = time.time()\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","\n","        metrics = process_frame_single(model, frame)\n","        frame_latency = metrics['pre_time'] + metrics['infer_time'] + metrics['post_time']\n","\n","        total_pre += metrics['pre_time']\n","        total_infer += metrics['infer_time']\n","        total_post += metrics['post_time']\n","        total_latency += frame_latency\n","        frame_count += 1\n","\n","    end_time = time.time()\n","    avg_fps = frame_count / (end_time - start_time)\n","    cpu_usage = psutil.cpu_percent()\n","    gpu_usage = (\n","        torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100\n","        if torch.cuda.is_available() else 0\n","    )\n","\n","    print(\"\\n=== Single Thread Performance (YOLOv8) ===\")\n","    print(f\"Total Frames Processed: {frame_count}\")\n","    print(f\"Average FPS: {avg_fps:.2f}\")\n","    print(f\"Preprocessing Latency: {total_pre/frame_count:.2f} ms\")\n","    print(f\"Inference Latency: {total_infer/frame_count:.2f} ms\")\n","    print(f\"Postprocessing Latency: {total_post/frame_count:.2f} ms\")\n","    print(f\"Total Frame Latency: {total_latency/frame_count:.2f} ms\")\n","    print(f\"CPU Utilization: {cpu_usage:.2f}%\")\n","    print(f\"GPU Utilization: {gpu_usage:.2f}%\")\n","\n","    return {\n","        \"fps\": avg_fps,\n","        \"pre\": total_pre / frame_count,\n","        \"infer\": total_infer / frame_count,\n","        \"post\": total_post / frame_count,\n","        \"total\": total_latency / frame_count,\n","        \"cpu\": cpu_usage,\n","        \"gpu\": gpu_usage\n","    }\n","\n","# ✅ Wrapper function to use `source=...` format\n","def run(source):\n","    video_path = source\n","    print(f\"Running inference on: {video_path}\")\n","\n","    # Load YOLOv8 model (change to yolov8s.pt, yolov8m.pt or your own model if needed)\n","    model = YOLO(\"yolov8n.pt\")\n","    return run_single_thread(video_path, model)\n","\n","# ✅ Example usage\n","run(source=\"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video2.mp4\")"]},{"cell_type":"markdown","metadata":{"id":"hYMZDs_JEWLP"},"source":["Multi threading"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":198416,"status":"ok","timestamp":1752416059869,"user":{"displayName":"kemi phd","userId":"00656621715362041141"},"user_tz":-120},"id":"2NYZR7OhDpNr","outputId":"61e81c5e-81b5-4d3e-be2b-9e82095e72b7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","\n","=== Multi-Thread Performance (YOLOv8) ===\n","Total Frames Processed: 714\n","Average FPS: 3.75 frames/s\n","Preprocessing Latency: 2.29 ms\n","Inference Latency: 1062.79 ms\n","Postprocessing Latency: 0.05 ms\n","Total Frame Latency: 1065.14 ms\n","CPU Utilization: 71.30%\n","GPU Utilization: 0.00%\n"]}],"source":["# ✅ INSTALL & IMPORTS\n","!pip install -q ultralytics psutil opencv-python\n","\n","import cv2\n","import time\n","import psutil\n","from ultralytics import YOLO\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","import torch\n","\n","# ✅ LOAD YOLOv8 MODEL (change 'yolov8n' to 'yolov8s' or your custom path if needed)\n","model = YOLO('yolov8n.pt')  # or use 'yolov8s.pt', 'yolov8m.pt', or 'path/to/custom.pt'\n","\n","# ✅ FRAME PROCESSING FUNCTION\n","def process_frame_single(model, frame):\n","    result = {}\n","\n","    pre_start = time.time()\n","    img = cv2.resize(frame, (640, 640))  # YOLOv8 prefers 640x640 by default\n","    pre_end = time.time()\n","\n","    infer_start = time.time()\n","    results = model.predict(img, verbose=False)\n","    infer_end = time.time()\n","\n","    post_start = time.time()\n","    _ = results[0].boxes.xyxy  # Access result to simulate postprocessing\n","    post_end = time.time()\n","\n","    result['pre_time'] = (pre_end - pre_start) * 1000\n","    result['infer_time'] = (infer_end - infer_start) * 1000\n","    result['post_time'] = (post_end - post_start) * 1000\n","    return result\n","\n","# ✅ MULTI-THREAD FUNCTION\n","def run_multi_thread(video_path, model):\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","    frames = []\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frames.append(frame)\n","\n","    total_pre, total_infer, total_post, total_latency = 0, 0, 0, 0\n","    start_time = time.time()\n","\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        futures = [executor.submit(process_frame_single, model, f) for f in frames]\n","\n","        for future in as_completed(futures):\n","            metrics = future.result()\n","            frame_latency = metrics['pre_time'] + metrics['infer_time'] + metrics['post_time']\n","            total_pre += metrics['pre_time']\n","            total_infer += metrics['infer_time']\n","            total_post += metrics['post_time']\n","            total_latency += frame_latency\n","            frame_count += 1\n","\n","    end_time = time.time()\n","    avg_fps = frame_count / (end_time - start_time)\n","    cpu_usage = psutil.cpu_percent()\n","    gpu_usage = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100 if torch.cuda.is_available() else 0\n","\n","    print(\"\\n=== Multi-Thread Performance (YOLOv8) ===\")\n","    print(f\"Total Frames Processed: {frame_count}\")\n","    print(f\"Average FPS: {avg_fps:.2f} frames/s\")\n","    print(f\"Preprocessing Latency: {total_pre/frame_count:.2f} ms\")\n","    print(f\"Inference Latency: {total_infer/frame_count:.2f} ms\")\n","    print(f\"Postprocessing Latency: {total_post/frame_count:.2f} ms\")\n","    print(f\"Total Frame Latency: {total_latency/frame_count:.2f} ms\")\n","    print(f\"CPU Utilization: {cpu_usage:.2f}%\")\n","    print(f\"GPU Utilization: {gpu_usage:.2f}%\")\n","\n","# ✅ RUN ON YOUR VIDEO\n","video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video2.mp4\"\n","run_multi_thread(video_path, model)"]},{"cell_type":"code","source":["import cv2\n","import time\n","import psutil\n","from ultralytics import YOLO\n","import torch\n","from multiprocessing import Process, Queue, cpu_count\n","import multiprocessing as mp\n","\n","# ✅ Load YOLOv8 model once in each process\n","def yolo_worker(input_queue, output_queue):\n","    model = YOLO('yolov8n.pt')\n","    while True:\n","        frame_id, frame = input_queue.get()\n","        if frame is None:\n","            break\n","\n","        result = {}\n","        pre_start = time.time()\n","        img = cv2.resize(frame, (640, 640))\n","        pre_end = time.time()\n","\n","        infer_start = time.time()\n","        results = model.predict(img, verbose=False)\n","        infer_end = time.time()\n","\n","        post_start = time.time()\n","        _ = results[0].boxes.xyxy\n","        post_end = time.time()\n","\n","        result['id'] = frame_id\n","        result['pre_time'] = (pre_end - pre_start) * 1000\n","        result['infer_time'] = (infer_end - infer_start) * 1000\n","        result['post_time'] = (post_end - post_start) * 1000\n","        output_queue.put(result)\n","\n","# ✅ Main video processing function using multiprocessing\n","def run_multi_process(video_path):\n","    input_queue = Queue(maxsize=10)\n","    output_queue = Queue()\n","\n","    num_workers = min(cpu_count(), 4)\n","    workers = [Process(target=yolo_worker, args=(input_queue, output_queue)) for _ in range(num_workers)]\n","\n","    for w in workers:\n","        w.start()\n","\n","    cap = cv2.VideoCapture(video_path)\n","    frame_id = 0\n","    active_frames = 0\n","\n","    total_pre, total_infer, total_post, total_latency = 0, 0, 0, 0\n","    start_time = time.time()\n","\n","    while True:\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        input_queue.put((frame_id, frame))\n","        active_frames += 1\n","        frame_id += 1\n","\n","    # Send stop signal\n","    for _ in workers:\n","        input_queue.put((None, None))\n","\n","    # Collect results\n","    for _ in range(active_frames):\n","        metrics = output_queue.get()\n","        frame_latency = metrics['pre_time'] + metrics['infer_time'] + metrics['post_time']\n","        total_pre += metrics['pre_time']\n","        total_infer += metrics['infer_time']\n","        total_post += metrics['post_time']\n","        total_latency += frame_latency\n","\n","    end_time = time.time()\n","    frame_count = active_frames\n","    avg_fps = frame_count / (end_time - start_time)\n","    cpu_usage = psutil.cpu_percent()\n","    gpu_usage = torch.cuda.memory_allocated() / torch.cuda.get_device_properties(0).total_memory * 100 if torch.cuda.is_available() else 0\n","\n","    print(\"\\n=== Multi-Processing Performance (YOLOv8) ===\")\n","    print(f\"Total Frames Processed: {frame_count}\")\n","    print(f\"Average FPS: {avg_fps:.2f} frames/s\")\n","    print(f\"Preprocessing Latency: {total_pre/frame_count:.2f} ms\")\n","    print(f\"Inference Latency: {total_infer/frame_count:.2f} ms\")\n","    print(f\"Postprocessing Latency: {total_post/frame_count:.2f} ms\")\n","    print(f\"Total Frame Latency: {total_latency/frame_count:.2f} ms\")\n","    print(f\"CPU Utilization: {cpu_usage:.2f}%\")\n","    print(f\"GPU Utilization: {gpu_usage:.2f}%\")\n","\n","    for w in workers:\n","        w.join()\n","\n","# ✅ Run on your video\n","if __name__ == '__main__':\n","    mp.set_start_method('spawn', force=True)  # safer for multiprocessing in some environments\n","    video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video2.mp4\"\n","    run_multi_process(video_path)\n"],"metadata":{"id":"ZILBHX6IHxG2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pifT3_nxvXYF"},"source":["Experiment Three"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wmw2Vt51MPMO","outputId":"448bbfdd-3be1-4332-d77d-c75780e276ef","executionInfo":{"status":"ok","timestamp":1752419176229,"user_tz":-120,"elapsed":402555,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","WARNING ⚠️ not enough matching points\n","\n","Thresholding Experiment Results:\n","       Metric  Static  Adaptive (Otsu)  Adaptive (Niblack)\n"," Accuracy (%)   14.86            16.00               14.29\n","Precision (%)   87.50            86.92              100.00\n","   Recall (%)   10.50            18.83                2.83\n"," F1-Score (%)   18.75            30.96                5.51\n","          MAE    2.05             1.97                2.12\n"]}],"source":["import cv2\n","import numpy as np\n","from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n","from skimage.filters import threshold_niblack\n","import pandas as pd\n","from pathlib import Path\n","\n","# Define the thresholding function\n","def apply_threshold(img, method='fixed', **kwargs):\n","    if method == 'fixed':\n","        _, thresh = cv2.threshold(img, kwargs.get('thresh', 127), 255, cv2.THRESH_BINARY)\n","    elif method == 'otsu':\n","        _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","    elif method == 'niblack':\n","        window_size = kwargs.get('window_size', 25)\n","        k = kwargs.get('k', 0.8)\n","        niblack_thresh = threshold_niblack(img, window_size=window_size, k=k)\n","        thresh = (img > niblack_thresh).astype(np.uint8) * 255\n","    else:\n","        raise ValueError(f\"Unknown thresholding method: {method}\")\n","    return thresh\n","\n","# Function to process a frame with thresholding and YOLOv8 detection\n","def process_frame_with_threshold(model, frame, method='fixed', **kwargs):\n","    try:\n","        # Convert frame to grayscale for thresholding\n","        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","\n","        # Apply thresholding\n","        thresh = apply_threshold(gray, method=method, **kwargs)\n","\n","        # Convert thresholded image back to 3 channels for YOLOv8 compatibility\n","        thresh_3ch = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n","\n","        # Run YOLOv8 detection\n","        results = model.track(thresh_3ch, persist=True, classes=[0], verbose=False)  # Class 0 for 'person'\n","\n","        # Count detected objects (persons)\n","        count = len(results[0].boxes) if results[0].boxes is not None else 0\n","        return count\n","    except Exception as e:\n","        print(f\"Error processing frame with {method} thresholding: {e}\")\n","        return 0\n","\n","# Function to evaluate thresholding methods\n","def evaluate_thresholding_methods(video_path, gt_counts, model_path=\"yolov8n.pt\"):\n","    # Check if video file exists\n","    if not Path(video_path).exists():\n","        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n","\n","    # Load YOLOv8 model\n","    model = YOLO(model_path)\n","\n","    # Initialize lists to store predicted counts for each method\n","    pred_counts_fixed = []\n","    pred_counts_otsu = []\n","    pred_counts_niblack = []\n","\n","    # Open video\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame_count += 1\n","\n","        # Process frame with each thresholding method\n","        count_fixed = process_frame_with_threshold(model, frame, method='fixed', thresh=127)\n","        count_otsu = process_frame_with_threshold(model, frame, method='otsu')\n","        count_niblack = process_frame_with_threshold(model, frame, method='niblack', window_size=25, k=0.8)\n","\n","        pred_counts_fixed.append(count_fixed)\n","        pred_counts_otsu.append(count_otsu)\n","        pred_counts_niblack.append(count_niblack)\n","\n","    cap.release()\n","\n","    # Ensure ground truth and predictions have the same length\n","    min_len = min(len(gt_counts), frame_count)\n","    if min_len == 0:\n","        raise ValueError(\"No frames processed or no ground truth data provided\")\n","\n","    gt_counts = gt_counts[:min_len]\n","    pred_counts_fixed = pred_counts_fixed[:min_len]\n","    pred_counts_otsu = pred_counts_otsu[:min_len]\n","    pred_counts_niblack = pred_counts_niblack[:min_len]\n","\n","    # Calculate metrics for each method\n","    def compute_metrics(gt, pred):\n","        binary_gt = (np.array(gt) > 0).astype(int)\n","        binary_pred = (np.array(pred) > 0).astype(int)\n","        accuracy = np.mean(np.array(gt) == np.array(pred)) * 100\n","        precision = precision_score(binary_gt, binary_pred, zero_division=0) * 100\n","        recall = recall_score(binary_gt, binary_pred, zero_division=0) * 100\n","        f1 = f1_score(binary_gt, binary_pred, zero_division=0) * 100\n","        mae = mean_absolute_error(gt, pred)\n","        return {\n","            \"Accuracy (%)\": round(accuracy, 2),\n","            \"Precision (%)\": round(precision, 2),\n","            \"Recall (%)\": round(recall, 2),\n","            \"F1-Score (%)\": round(f1, 2),\n","            \"MAE\": round(mae, 2)\n","        }\n","\n","    metrics_fixed = compute_metrics(gt_counts, pred_counts_fixed)\n","    metrics_otsu = compute_metrics(gt_counts, pred_counts_otsu)\n","    metrics_niblack = compute_metrics(gt_counts, pred_counts_niblack)\n","\n","    # Create a DataFrame to display results\n","    results = pd.DataFrame({\n","        \"Metric\": [\"Accuracy (%)\", \"Precision (%)\", \"Recall (%)\", \"F1-Score (%)\", \"MAE\"],\n","        \"Static\": [metrics_fixed[m] for m in metrics_fixed],\n","        \"Adaptive (Otsu)\": [metrics_otsu[m] for m in metrics_otsu],\n","        \"Adaptive (Niblack)\": [metrics_niblack[m] for m in metrics_niblack]\n","    })\n","\n","    print(\"\\nThresholding Experiment Results:\")\n","    print(results.to_string(index=False))\n","\n","    return results\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    try:\n","        # Replace with your video path and ground truth counts\n","        video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video2.mp4\"\n","        # Example ground truth counts (replace with actual data)\n","        gt_counts = [2, 3, 4, 2, 0, 1, 3] * 100  # Repeated to approximate video length\n","        results = evaluate_thresholding_methods(video_path, gt_counts)\n","    except Exception as e:\n","        print(f\"Error running experiment: {e}\")"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n","from pathlib import Path\n","import pandas as pd\n","from ultralytics import YOLO\n","\n","# CLAHE + optional threshold mask\n","def enhance_and_mask(frame, apply_mask=False, method='otsu', **kwargs):\n","    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    enhanced = clahe.apply(gray)\n","\n","    if apply_mask:\n","        if method == 'otsu':\n","            _, mask = cv2.threshold(enhanced, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","        else:\n","            raise ValueError(f\"Unsupported mask method: {method}\")\n","        masked = cv2.bitwise_and(frame, frame, mask=mask)\n","        return masked\n","    else:\n","        return cv2.cvtColor(enhanced, cv2.COLOR_GRAY2BGR)\n","\n","# Process a frame using enhanced contrast (with optional masking)\n","def process_frame(model, frame, use_mask=False, **kwargs):\n","    try:\n","        enhanced_frame = enhance_and_mask(frame, apply_mask=use_mask, **kwargs)\n","        results = model.track(enhanced_frame, persist=True, classes=[0], conf=0.5, verbose=False)\n","        count = len(results[0].boxes) if results[0].boxes is not None else 0\n","        return count\n","    except Exception as e:\n","        print(f\"Error processing frame: {e}\")\n","        return 0\n","\n","# Evaluate improved method\n","def evaluate_method(video_path, gt_counts, model_path=\"yolov8n.pt\", use_mask=False):\n","    if not Path(video_path).exists():\n","        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n","\n","    model = YOLO(model_path)\n","    pred_counts = []\n","    cap = cv2.VideoCapture(video_path)\n","    frame_count = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frame_count += 1\n","        count = process_frame(model, frame, use_mask=use_mask)\n","        pred_counts.append(count)\n","\n","    cap.release()\n","\n","    min_len = min(len(gt_counts), len(pred_counts))\n","    gt_counts = gt_counts[:min_len]\n","    pred_counts = pred_counts[:min_len]\n","\n","    binary_gt = (np.array(gt_counts) > 0).astype(int)\n","    binary_pred = (np.array(pred_counts) > 0).astype(int)\n","\n","    accuracy = 100 - (mean_absolute_error(gt_counts, pred_counts) / max(max(gt_counts), 1)) * 100\n","    precision = precision_score(binary_gt, binary_pred, zero_division=0) * 100\n","    recall = recall_score(binary_gt, binary_pred, zero_division=0) * 100\n","    f1 = f1_score(binary_gt, binary_pred, zero_division=0) * 100\n","    mae = mean_absolute_error(gt_counts, pred_counts)\n","\n","    results_df = pd.DataFrame({\n","        \"Metric\": [\"Accuracy (%)\", \"Precision (%)\", \"Recall (%)\", \"F1-Score (%)\", \"MAE\"],\n","        \"CLAHE + YOLOv8\": [round(accuracy, 2), round(precision, 2), round(recall, 2), round(f1, 2), round(mae, 2)]\n","    })\n","\n","    print(\"\\nImproved Detection Results (CLAHE + YOLOv8):\")\n","    print(results_df.to_string(index=False))\n","    return results_df\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    try:\n","        video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video2.mp4\"\n","        gt_counts = [2, 3, 4, 2, 0, 1, 3] * 100  # Replace with actual data\n","        results = evaluate_method(video_path, gt_counts, use_mask=False)\n","    except Exception as e:\n","        print(f\"Experiment error: {e}\")"],"metadata":{"id":"wE9Odv8kbPkW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1752418543104,"user_tz":-120,"elapsed":146358,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}},"outputId":"8a04b966-e9dd-4660-da18-3da78d45f2ba"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 6.25M/6.25M [00:00<00:00, 66.2MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['lap>=0.5.12'] not found, attempting AutoUpdate...\n","\n","\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ✅ 1.0s\n","WARNING ⚠️ \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n","\n","\n","Improved Detection Results (CLAHE + YOLOv8):\n","       Metric  CLAHE + YOLOv8\n"," Accuracy (%)           27.18\n","Precision (%)           85.45\n","   Recall (%)           92.00\n"," F1-Score (%)           88.60\n","          MAE            2.91\n"]}]},{"cell_type":"markdown","metadata":{"id":"bSOw--lB3rD7"},"source":["Experiment 4"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69qT0JqTMP6l","outputId":"b16a474c-64ca-4763-9470-dda36c056c3e","executionInfo":{"status":"ok","timestamp":1752421570931,"user_tz":-120,"elapsed":94178,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Running Full System...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","Error running ablation study: operands could not be broadcast together with shapes (700,) (401,) \n"]}],"source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n","from skimage.filters import threshold_niblack\n","import pandas as pd\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","import json\n","import time\n","\n","# Thresholding function (Otsu's method for full system)\n","def apply_threshold(img, method='otsu', **kwargs):\n","    if method == 'otsu':\n","        _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","        return thresh\n","    elif method == 'none':\n","        return img  # No thresholding\n","    else:\n","        raise ValueError(f\"Unknown thresholding method: {method}\")\n","\n","# Process a single frame with optional thresholding and augmentation\n","def process_frame(model, frame, use_threshold=True, use_augmentation=False):\n","    try:\n","        # Apply augmentation if enabled (simulated with random brightness/contrast)\n","        img = frame.copy()\n","        if use_augmentation:\n","            img = cv2.convertScaleAbs(img, alpha=np.random.uniform(0.8, 1.2), beta=np.random.randint(-20, 20))\n","\n","        # Apply thresholding if enabled\n","        if use_threshold:\n","            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","            thresh = apply_threshold(gray, method='otsu')\n","            img = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n","\n","        # Run YOLOv8 detection\n","        results = model.track(img, persist=True, classes=[0], verbose=False)  # Class 0 for 'person'\n","\n","        # Count objects and collect bounding boxes\n","        count = len(results[0].boxes) if results[0].boxes is not None else 0\n","        boxes = results[0].boxes.xyxy.cpu().numpy().tolist() if results[0].boxes is not None else []\n","        scores = results[0].boxes.conf.cpu().numpy().tolist() if results[0].boxes is not None else []\n","        return count, boxes, scores\n","    except Exception as e:\n","        print(f\"Error processing frame: {e}\")\n","        return 0, [], []\n","\n","# Single-threaded processing\n","def process_video_single_thread(video_path, model, use_threshold=True, use_augmentation=False):\n","    cap = cv2.VideoCapture(video_path)\n","    counts, all_boxes, all_scores = [], [], []\n","    frame_id = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        count, boxes, scores = process_frame(model, frame, use_threshold, use_augmentation)\n","        counts.append(count)\n","        all_boxes.append(boxes)\n","        all_scores.append(scores)\n","        frame_id += 1\n","\n","    cap.release()\n","    return counts, all_boxes, all_scores, frame_id\n","\n","# Multi-threaded processing\n","def process_video_multi_thread(video_path, model, use_threshold=True, use_augmentation=False):\n","    cap = cv2.VideoCapture(video_path)\n","    frames, frame_ids = [], []\n","    frame_id = 0\n","\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frames.append(frame)\n","        frame_ids.append(frame_id)\n","        frame_id += 1\n","\n","    cap.release()\n","    counts = [0] * frame_id\n","    all_boxes = [[] for _ in range(frame_id)]\n","    all_scores = [[] for _ in range(frame_id)]\n","\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        futures = [executor.submit(process_frame, model, f, use_threshold, use_augmentation) for f in frames]\n","        for idx, future in enumerate(as_completed(futures)):\n","            count, boxes, scores = future.result()\n","            counts[frame_ids[idx]] = count\n","            all_boxes[frame_ids[idx]] = boxes\n","            all_scores[frame_ids[idx]] = scores\n","\n","    return counts, all_boxes, all_scores, frame_id\n","\n","# Compute metrics including mAP\n","def compute_metrics(gt_counts, pred_counts, gt_annotations, pred_boxes, pred_scores, frame_count):\n","    binary_gt = (np.array(gt_counts) > 0).astype(int)\n","    binary_pred = (np.array(pred_counts) > 0).astype(int)\n","    accuracy = np.mean(np.array(gt_counts) == np.array(pred_counts)) * 100\n","    precision = precision_score(binary_gt, binary_pred, zero_division=0) * 100\n","    recall = recall_score(binary_gt, binary_pred, zero_division=0) * 100\n","    f1 = f1_score(binary_gt, binary_pred, zero_division=0) * 100\n","    mae = mean_absolute_error(gt_counts, pred_counts)\n","\n","    # Prepare COCO format for mAP\n","    coco_gt = {\n","        \"images\": [{\"id\": i, \"width\": 640, \"height\": 352} for i in range(frame_count)],\n","        \"annotations\": gt_annotations,\n","        \"categories\": [{\"id\": 0, \"name\": \"person\"}]\n","    }\n","    with open(\"gt.json\", \"w\") as f:\n","        json.dump(coco_gt, f)\n","\n","    coco_dt = []\n","    ann_id = 1\n","    for img_id, (boxes, scores) in enumerate(zip(pred_boxes, pred_scores)):\n","        for box, score in zip(boxes, scores):\n","            x, y, x2, y2 = box\n","            coco_dt.append({\n","                \"image_id\": img_id,\n","                \"category_id\": 0,\n","                \"bbox\": [x, y, x2 - x, y2 - y],\n","                \"score\": score,\n","                \"id\": ann_id\n","            })\n","            ann_id += 1\n","    with open(\"dt.json\", \"w\") as f:\n","        json.dump(coco_dt, f)\n","\n","    coco_gt = COCO(\"gt.json\")\n","    coco_dt = coco_gt.loadRes(\"dt.json\")\n","    coco_eval = COCOeval(coco_gt, coco_dt, \"bbox\")\n","    coco_eval.evaluate()\n","    coco_eval.accumulate()\n","    coco_eval.summarize()\n","    map_score = coco_eval.stats[0] * 100  # mAP@0.5\n","\n","    return {\n","        \"Accuracy (%)\": round(accuracy, 2),\n","        \"F1-Score (%)\": round(f1, 2),\n","        \"mAP (%)\": round(map_score, 2),\n","        \"MAE\": round(mae, 2)\n","    }\n","\n","# Main ablation study function\n","def run_ablation_study(video_path, gt_counts, gt_annotations, fine_tuned_model_path=\"yolov8n.pt\"):\n","    if not Path(video_path).exists():\n","        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n","\n","    results = []\n","\n","    # Full System: Multithreading, Otsu thresholding, augmentation, fine-tuned model\n","    print(\"Running Full System...\")\n","    model = YOLO(fine_tuned_model_path)\n","    counts, boxes, scores, frame_count = process_video_multi_thread(video_path, model, use_threshold=True, use_augmentation=True)\n","    metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","    results.append({\"Configuration\": \"Full System\", **metrics})\n","\n","    # w/o Multithreading: Single-threaded, keep others\n","    print(\"Running w/o Multithreading...\")\n","    model = YOLO(fine_tuned_model_path)\n","    counts, boxes, scores, frame_count = process_video_single_thread(video_path, model, use_threshold=True, use_augmentation=True)\n","    metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","    results.append({\"Configuration\": \"w/o Multithreading\", **metrics})\n","\n","    # w/o Adaptive Thresholding: Multithreading, no thresholding, keep others\n","    print(\"Running w/o Adaptive Thresholding...\")\n","    model = YOLO(fine_tuned_model_path)\n","    counts, boxes, scores, frame_count = process_video_multi_thread(video_path, model, use_threshold=False, use_augmentation=True)\n","    metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","    results.append({\"Configuration\": \"w/o Adaptive Thresholding\", **metrics})\n","\n","    # w/o Data Augmentation: Multithreading, thresholding, no augmentation\n","    print(\"Running w/o Data Augmentation...\")\n","    model = YOLO(fine_tuned_model_path)\n","    counts, boxes, scores, frame_count = process_video_multi_thread(video_path, model, use_threshold=True, use_augmentation=False)\n","    metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","    results.append({\"Configuration\": \"w/o Data Augmentation\", **metrics})\n","\n","    # w/o Fine-Tuning: Multithreading, thresholding, augmentation, pre-trained model\n","    print(\"Running w/o Fine-Tuning...\")\n","    model = YOLO(\"yolov8n.pt\")\n","    counts, boxes, scores, frame_count = process_video_multi_thread(video_path, model, use_threshold=True, use_augmentation=True)\n","    metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","    results.append({\"Configuration\": \"w/o Fine-Tuning\", **metrics})\n","\n","    # Create results DataFrame\n","    results_df = pd.DataFrame(results)\n","    print(\"\\nAblation Study Results:\")\n","    print(results_df.to_string(index=False))\n","\n","    return results_df\n","\n","# Example usage\n","if __name__ == \"__main__\":\n","    try:\n","        video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_videoAnnotated.mp4\"\n","        # Example ground truth counts (replace with actual data)\n","        gt_counts = [2, 3, 4, 2, 0, 1, 3] * 100  # Repeated to approximate 714 frames\n","        # Example ground truth annotations for mAP (replace with actual COCO-format annotations)\n","        gt_annotations = [\n","            {\"id\": i+1, \"image_id\": i//7, \"category_id\": 0, \"bbox\": [100, 100, 50, 50], \"area\": 2500}\n","            for i in range(len(gt_counts)) if gt_counts[i] > 0\n","        ]\n","        # Replace with path to your fine-tuned model\n","        fine_tuned_model_path = \"yolov8n.pt\"\n","        results = run_ablation_study(video_path, gt_counts, gt_annotations, fine_tuned_model_path)\n","    except Exception as e:\n","        print(f\"Error running ablation study: {e}\")"]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n","import pandas as pd\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","import json\n","import time\n","import os\n","\n","def apply_threshold(img, method='otsu'):\n","    if method == 'otsu':\n","        _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","        return thresh\n","    elif method == 'none':\n","        return img\n","    else:\n","        raise ValueError(f\"Unknown thresholding method: {method}\")\n","\n","def process_frame(model, frame, use_threshold=True, use_augmentation=False):\n","    try:\n","        img = frame.copy()\n","        if use_augmentation:\n","            img = cv2.convertScaleAbs(img, alpha=np.random.uniform(0.8, 1.2), beta=np.random.randint(-20, 20))\n","        if use_threshold:\n","            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","            thresh = apply_threshold(gray, method='otsu')\n","            img = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR)\n","        results = model.track(img, persist=True, classes=[0], verbose=False)\n","        count = len(results[0].boxes) if results[0].boxes is not None else 0\n","        boxes = results[0].boxes.xyxy.cpu().numpy().tolist() if results[0].boxes is not None else []\n","        scores = results[0].boxes.conf.cpu().numpy().tolist() if results[0].boxes is not None else []\n","        return count, boxes, scores\n","    except Exception as e:\n","        print(f\"Error processing frame: {e}\")\n","        return 0, [], []\n","\n","def process_video_single_thread(video_path, model, use_threshold=True, use_augmentation=False):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise ValueError(f\"Failed to open video: {video_path}\")\n","    counts, all_boxes, all_scores = [], [], []\n","    frame_id = 0\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        count, boxes, scores = process_frame(model, frame, use_threshold, use_augmentation)\n","        counts.append(count)\n","        all_boxes.append(boxes)\n","        all_scores.append(scores)\n","        frame_id += 1\n","    cap.release()\n","    return counts, all_boxes, all_scores, frame_id\n","\n","def process_video_multi_thread(video_path, model, use_threshold=True, use_augmentation=False):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise ValueError(f\"Failed to open video: {video_path}\")\n","    frames, frame_ids = [], []\n","    frame_id = 0\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frames.append(frame)\n","        frame_ids.append(frame_id)\n","        frame_id += 1\n","    cap.release()\n","    counts = [0] * frame_id\n","    all_boxes = [[] for _ in range(frame_id)]\n","    all_scores = [[] for _ in range(frame_id)]\n","\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        futures = [executor.submit(process_frame, model, f, use_threshold, use_augmentation) for f in frames]\n","        for idx, future in enumerate(as_completed(futures)):\n","            count, boxes, scores = future.result()\n","            counts[frame_ids[idx]] = count\n","            all_boxes[frame_ids[idx]] = boxes\n","            all_scores[frame_ids[idx]] = scores\n","    return counts, all_boxes, all_scores, frame_id\n","\n","def compute_metrics(gt_counts, pred_counts, gt_annotations, pred_boxes, pred_scores, frame_count):\n","    try:\n","        # Align lengths\n","        min_len = min(len(gt_counts), len(pred_counts))\n","        gt_counts = gt_counts[:min_len]\n","        pred_counts = pred_counts[:min_len]\n","\n","        binary_gt = (np.array(gt_counts) > 0).astype(int)\n","        binary_pred = (np.array(pred_counts) > 0).astype(int)\n","        accuracy = np.mean(np.array(gt_counts) == np.array(pred_counts)) * 100\n","        precision = precision_score(binary_gt, binary_pred, zero_division=0) * 100\n","        recall = recall_score(binary_gt, binary_pred, zero_division=0) * 100\n","        f1 = f1_score(binary_gt, binary_pred, zero_division=0) * 100\n","        mae = mean_absolute_error(gt_counts, pred_counts)\n","\n","        # Prepare COCO format\n","        coco_gt = {\n","            \"images\": [{\"id\": i, \"width\": 640, \"height\": 352} for i in range(min_len)],\n","            \"annotations\": [ann for ann in gt_annotations if ann[\"image_id\"] < min_len],\n","            \"categories\": [{\"id\": 0, \"name\": \"person\"}]\n","        }\n","        gt_file = \"gt.json\"\n","        with open(gt_file, \"w\") as f:\n","            json.dump(coco_gt, f)\n","\n","        coco_dt = []\n","        ann_id = 1\n","        for img_id, (boxes, scores) in enumerate(zip(pred_boxes, pred_scores)):\n","            if img_id >= min_len:\n","                break\n","            for box, score in zip(boxes, scores):\n","                x, y, x2, y2 = box\n","                coco_dt.append({\n","                    \"image_id\": img_id,\n","                    \"category_id\": 0,\n","                    \"bbox\": [x, y, x2 - x, y2 - y],\n","                    \"score\": float(score),  # Ensure score is float\n","                    \"id\": ann_id\n","                })\n","                ann_id += 1\n","        dt_file = \"dt.json\"\n","        with open(dt_file, \"w\") as f:\n","            json.dump(coco_dt, f)\n","\n","        # Load COCO ground truth and detections\n","        coco_gt_obj = COCO(gt_file)\n","        if not coco_dt:\n","            print(\"Warning: No detections found, skipping COCO evaluation.\")\n","            return {\n","                \"Accuracy (%)\": round(accuracy, 2),\n","                \"Precision (%)\": round(precision, 2),\n","                \"Recall (%)\": round(recall, 2),\n","                \"F1-Score (%)\": round(f1, 2),\n","                \"mAP (%)\": 0.0,\n","                \"MAE\": round(mae, 2)\n","            }\n","\n","        coco_dt_obj = coco_gt_obj.loadRes(dt_file)\n","        coco_eval = COCOeval(coco_gt_obj, coco_dt_obj, \"bbox\")\n","        coco_eval.evaluate()\n","        coco_eval.accumulate()\n","        coco_eval.summarize()\n","        map_score = coco_eval.stats[0] * 100 if coco_eval.stats is not None else 0.0\n","\n","        # Clean up temporary files\n","        for file in [gt_file, dt_file]:\n","            if os.path.exists(file):\n","                os.remove(file)\n","\n","        return {\n","            \"Accuracy (%)\": round(accuracy, 2),\n","            \"Precision (%)\": round(precision, 2),\n","            \"Recall (%)\": round(recall, 2),\n","            \"F1-Score (%)\": round(f1, 2),\n","            \"mAP (%)\": round(map_score, 2),\n","            \"MAE\": round(mae, 2)\n","        }\n","    except Exception as e:\n","        print(f\"Error in compute_metrics: {e}\")\n","        return {\n","            \"Accuracy (%)\": round(accuracy, 2),\n","            \"Precision (%)\": round(precision, 2),\n","            \"Recall (%)\": round(recall, 2),\n","            \"F1-Score (%)\": round(f1, 2),\n","            \"mAP (%)\": 0.0,\n","            \"MAE\": round(mae, 2)\n","        }\n","\n","def run_ablation_study(video_path, gt_counts, gt_annotations, fine_tuned_model_path=\"yolov8n.pt\"):\n","    if not Path(video_path).exists():\n","        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n","\n","    # Load the model once and reuse it\n","    model = YOLO(fine_tuned_model_path)\n","    results = []\n","\n","    configurations = [\n","        (\"Full System\", True, True, True),\n","        (\"w/o Multithreading\", True, True, False),\n","        (\"w/o Adaptive Thresholding\", False, True, True),\n","        (\"w/o Data Augmentation\", True, False, True),\n","    ]\n","\n","    for config_name, use_threshold, use_augmentation, use_multithreading in configurations:\n","        print(f\"Running {config_name}...\")\n","        process_func = process_video_multi_thread if use_multithreading else process_video_single_thread\n","        counts, boxes, scores, frame_count = process_func(video_path, model, use_threshold, use_augmentation)\n","        metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","        results.append({\"Configuration\": config_name, **metrics})\n","\n","    # Run w/o Fine-Tuning with a separate model\n","    print(\"Running w/o Fine-Tuning...\")\n","    model_base = YOLO(\"yolov8n.pt\")\n","    counts, boxes, scores, frame_count = process_video_multi_thread(video_path, model_base, True, True)\n","    metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","    results.append({\"Configuration\": \"w/o Fine-Tuning\", **metrics})\n","\n","    results_df = pd.DataFrame(results)\n","    print(\"\\nAblation Study Results:\")\n","    print(results_df.to_string(index=False))\n","    return results_df\n","\n","# Run if standalone\n","if __name__ == \"__main__\":\n","    try:\n","        video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video.mp4\"\n","        gt_counts = [2, 3, 4, 2, 0, 1, 3] * 100  # 700 synthetic\n","        gt_annotations = [{\"id\": i+1, \"image_id\": i//7, \"category_id\": 0, \"bbox\": [100, 100, 50, 50], \"area\": 2500}\n","                          for i in range(len(gt_counts)) if gt_counts[i] > 0]\n","        fine_tuned_model_path = \"yolov8n.pt\"\n","        results = run_ablation_study(video_path, gt_counts, gt_annotations, fine_tuned_model_path)\n","    except Exception as e:\n","        print(f\"Error running ablation study: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nZAfNvI3kC-M","executionInfo":{"status":"ok","timestamp":1752423692781,"user_tz":-120,"elapsed":457011,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}},"outputId":"418195a5-33d4-4713-f835-c995db8321e8"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Full System...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Multithreading...\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Adaptive Thresholding...\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Data Augmentation...\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Fine-Tuning...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","\n","Ablation Study Results:\n","            Configuration  Accuracy (%)  Precision (%)  Recall (%)  F1-Score (%)  mAP (%)  MAE\n","              Full System         13.47          84.18       43.31         57.20      0.0 1.84\n","       w/o Multithreading         15.46          86.23       41.86         56.36      0.0 1.82\n","w/o Adaptive Thresholding         15.96          85.79      100.00         92.35      0.0 1.74\n","    w/o Data Augmentation         12.97          84.38       39.24         53.57      0.0 1.89\n","          w/o Fine-Tuning         13.47          85.88       44.19         58.35      0.0 1.83\n"]}]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","from ultralytics import YOLO\n","from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error\n","import pandas as pd\n","from pathlib import Path\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from pycocotools.coco import COCO\n","from pycocotools.cocoeval import COCOeval\n","import json\n","import time\n","import os\n","\n","def apply_threshold(img, method='none'):\n","    if method == 'otsu':\n","        _, thresh = cv2.threshold(img, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","        return thresh\n","    elif method == 'adaptive':\n","        return cv2.adaptiveThreshold(img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 11, 2)\n","    elif method == 'none':\n","        return img\n","    else:\n","        raise ValueError(f\"Unknown thresholding method: {method}\")\n","\n","def augment_image(img):\n","    if np.random.rand() > 0.5:\n","        img = cv2.flip(img, 1)  # Horizontal flip\n","    if np.random.rand() > 0.5:\n","        angle = np.random.uniform(-10, 10)\n","        h, w = img.shape[:2]\n","        M = cv2.getRotationMatrix2D((w/2, h/2), angle, 1)\n","        img = cv2.warpAffine(img, M, (w, h))\n","    img = cv2.convertScaleAbs(img, alpha=np.random.uniform(0.8, 1.2), beta=np.random.randint(-20, 20))\n","    return img\n","\n","def process_frame(model, frame, use_threshold=True, use_augmentation=False, conf=0.3):\n","    try:\n","        img = frame.copy()\n","        if use_augmentation:\n","            img = augment_image(img)\n","        if use_threshold:\n","            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","            thresh = apply_threshold(gray, method='adaptive' if use_threshold else 'none')\n","            img = cv2.cvtColor(thresh, cv2.COLOR_GRAY2BGR) if use_threshold else img\n","        results = model.track(img, persist=True, classes=[0], conf=conf, iou=0.5, verbose=False)\n","        count = len(results[0].boxes) if results[0].boxes is not None else 0\n","        boxes = results[0].boxes.xyxy.cpu().numpy().tolist() if results[0].boxes is not None else []\n","        scores = results[0].boxes.conf.cpu().numpy().tolist() if results[0].boxes is not None else []\n","        # Visualize for debugging\n","        if np.random.rand() < 0.01:  # Save 1% of frames\n","            visualize_frame(img, boxes, f\"output/frame_{time.time()}.jpg\")\n","        return count, boxes, scores\n","    except Exception as e:\n","        print(f\"Error processing frame: {e}\")\n","        return 0, [], []\n","\n","def visualize_frame(frame, boxes, output_path):\n","    img = frame.copy()\n","    for box in boxes:\n","        x, y, x2, y2 = map(int, box)\n","        cv2.rectangle(img, (x, y), (x2, y2), (0, 255, 0), 2)\n","    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n","    cv2.imwrite(output_path, img)\n","\n","def process_video_single_thread(video_path, model, use_threshold=True, use_augmentation=False, conf=0.3):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise ValueError(f\"Failed to open video: {video_path}\")\n","    counts, all_boxes, all_scores = [], [], []\n","    frame_id = 0\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        count, boxes, scores = process_frame(model, frame, use_threshold, use_augmentation, conf)\n","        counts.append(count)\n","        all_boxes.append(boxes)\n","        all_scores.append(scores)\n","        frame_id += 1\n","    cap.release()\n","    return counts, all_boxes, all_scores, frame_id\n","\n","def process_video_multi_thread(video_path, model, use_threshold=True, use_augmentation=False, conf=0.3):\n","    cap = cv2.VideoCapture(video_path)\n","    if not cap.isOpened():\n","        raise ValueError(f\"Failed to open video: {video_path}\")\n","    frames, frame_ids = [], []\n","    frame_id = 0\n","    while cap.isOpened():\n","        ret, frame = cap.read()\n","        if not ret:\n","            break\n","        frames.append(frame)\n","        frame_ids.append(frame_id)\n","        frame_id += 1\n","    cap.release()\n","    counts = [0] * frame_id\n","    all_boxes = [[] for _ in range(frame_id)]\n","    all_scores = [[] for _ in range(frame_id)]\n","\n","    with ThreadPoolExecutor(max_workers=4) as executor:\n","        futures = {executor.submit(process_frame, model, f, use_threshold, use_augmentation, conf): i for i, f in enumerate(frames)}\n","        for future in as_completed(futures):\n","            idx = futures[future]\n","            count, boxes, scores = future.result()\n","            counts[idx] = count\n","            all_boxes[idx] = boxes\n","            all_scores[idx] = scores\n","    return counts, all_boxes, all_scores, frame_id\n","\n","def compute_metrics(gt_counts, pred_counts, gt_annotations, pred_boxes, pred_scores, frame_count):\n","    try:\n","        min_len = min(len(gt_counts), len(pred_counts))\n","        gt_counts = gt_counts[:min_len]\n","        pred_counts = pred_counts[:min_len]\n","\n","        binary_gt = (np.array(gt_counts) > 0).astype(int)\n","        binary_pred = (np.array(pred_counts) > 0).astype(int)\n","        accuracy = np.mean(np.array(gt_counts) == np.array(pred_counts)) * 100\n","        precision = precision_score(binary_gt, binary_pred, zero_division=0) * 100\n","        recall = recall_score(binary_gt, binary_pred, zero_division=0) * 100\n","        f1 = f1_score(binary_gt, binary_pred, zero_division=0) * 100\n","        mae = mean_absolute_error(gt_counts, pred_counts)\n","\n","        coco_gt = {\n","            \"images\": [{\"id\": i, \"width\": 640, \"height\": 352} for i in range(min_len)],\n","            \"annotations\": [ann for ann in gt_annotations if ann[\"image_id\"] < min_len],\n","            \"categories\": [{\"id\": 0, \"name\": \"person\"}]\n","        }\n","        gt_file = \"gt.json\"\n","        with open(gt_file, \"w\") as f:\n","            json.dump(coco_gt, f)\n","\n","        coco_dt = []\n","        ann_id = 1\n","        for img_id, (boxes, scores) in enumerate(zip(pred_boxes, pred_scores)):\n","            if img_id >= min_len:\n","                continue\n","            for box, score in zip(boxes, scores):\n","                if len(box) != 4 or not all(isinstance(coord, (int, float)) for coord in box):\n","                    continue\n","                x, y, x2, y2 = box\n","                w, h = x2 - x, y2 - y\n","                if w <= 0 or h <= 0:\n","                    continue\n","                coco_dt.append({\n","                    \"image_id\": img_id,\n","                    \"category_id\": 0,\n","                    \"bbox\": [float(x), float(y), float(w), float(h)],\n","                    \"score\": float(min(max(score, 0), 1)),\n","                    \"id\": ann_id\n","                })\n","                ann_id += 1\n","\n","        print(f\"Ground truth annotations: {len(coco_gt['annotations'])}\")\n","        print(f\"Predicted detections: {len(coco_dt)}\")\n","\n","        if not coco_dt:\n","            print(\"Warning: No valid detections for COCO evaluation.\")\n","            return {\n","                \"Accuracy (%)\": round(accuracy, 2),\n","                \"Precision (%)\": round(precision, 2),\n","                \"Recall (%)\": round(recall, 2),\n","                \"F1-Score (%)\": round(f1, 2),\n","                \"mAP (%)\": 0.0,\n","                \"MAE\": round(mae, 2)\n","            }\n","\n","        dt_file = \"dt.json\"\n","        with open(dt_file, \"w\") as f:\n","            json.dump(coco_dt, f)\n","\n","        coco_gt_obj = COCO(gt_file)\n","        coco_dt_obj = coco_gt_obj.loadRes(dt_file)\n","        coco_eval = COCOeval(coco_gt_obj, coco_dt_obj, \"bbox\")\n","        coco_eval.params.imgIds = [i for i in range(min_len)]\n","        coco_eval.evaluate()\n","        coco_eval.accumulate()\n","        coco_eval.summarize()\n","        map_score = coco_eval.stats[0] * 100 if coco_eval.stats is not None else 0.0\n","\n","        for file in [gt_file, dt_file]:\n","            if os.path.exists(file):\n","                os.remove(file)\n","\n","        return {\n","            \"Accuracy (%)\": round(accuracy, 2),\n","            \"Precision (%)\": round(precision, 2),\n","            \"Recall (%)\": round(recall, 2),\n","            \"F1-Score (%)\": round(f1, 2),\n","            \"mAP (%)\": round(map_score, 2),\n","            \"MAE\": round(mae, 2)\n","        }\n","    except Exception as e:\n","        print(f\"Error in compute_metrics: {e}\")\n","        return {\n","            \"Accuracy (%)\": round(accuracy, 2),\n","            \"Precision (%)\": round(precision, 2),\n","            \"Recall (%)\": round(recall, 2),\n","            \"F1-Score (%)\": round(f1, 2),\n","            \"mAP (%)\": 0.0,\n","            \"MAE\": round(mae, 2)\n","        }\n","\n","def run_ablation_study(video_path, gt_counts, gt_annotations, fine_tuned_model_path=\"yolov8n.pt\"):\n","    if not Path(video_path).exists():\n","        raise FileNotFoundError(f\"Video file not found: {video_path}\")\n","\n","    configurations = [\n","        (\"Full System\", True, True, True, fine_tuned_model_path, 0.3),\n","        (\"w/o Multithreading\", True, True, False, fine_tuned_model_path, 0.3),\n","        (\"w/o Adaptive Thresholding\", False, True, True, fine_tuned_model_path, 0.3),\n","        (\"w/o Data Augmentation\", True, False, True, fine_tuned_model_path, 0.3),\n","        (\"Larger Model\", True, True, True, \"yolov8m.pt\", 0.3),\n","        (\"Lower Conf Threshold\", True, True, True, fine_tuned_model_path, 0.1),\n","    ]\n","\n","    results = []\n","    for config_name, use_threshold, use_augmentation, use_multithreading, model_path, conf in configurations:\n","        print(f\"Running {config_name}...\")\n","        model = YOLO(model_path)\n","        process_func = process_video_multi_thread if use_multithreading else process_video_single_thread\n","        counts, boxes, scores, frame_count = process_func(video_path, model, use_threshold, use_augmentation, conf)\n","        metrics = compute_metrics(gt_counts, counts, gt_annotations, boxes, scores, frame_count)\n","        results.append({\"Configuration\": config_name, **metrics})\n","\n","    results_df = pd.DataFrame(results)\n","    print(\"\\nAblation Study Results:\")\n","    print(results_df.to_string(index=False))\n","    return results_df\n","\n","if __name__ == \"__main__\":\n","    try:\n","        video_path = \"/content/drive/MyDrive/PhD Deep Learning Models/Computer Vision/Computer Vision/Test/test_video.mp4\"\n","        gt_counts = [2, 3, 4, 2, 0, 1, 3] * 100  # Replace with actual counts\n","        # Replace with realistic annotations from CVAT or similar\n","        gt_annotations = [{\"id\": i+1, \"image_id\": i, \"category_id\": 0, \"bbox\": [100, 100, 50, 50], \"area\": 2500}\n","                          for i in range(len(gt_counts)) if gt_counts[i] > 0]\n","        fine_tuned_model_path = \"yolov8n.pt\"  # Update with actual path\n","        results = run_ablation_study(video_path, gt_counts, gt_annotations, fine_tuned_model_path)\n","    except Exception as e:\n","        print(f\"Error running ablation study: {e}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N6hghbxXqOAE","executionInfo":{"status":"ok","timestamp":1752426051604,"user_tz":-120,"elapsed":913022,"user":{"displayName":"kemi phd","userId":"00656621715362041141"}},"outputId":"df3a1dc0-e10e-4a32-9966-1c92932c3c84"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Running Full System...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","Ground truth annotations: 344\n","Predicted detections: 11\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Multithreading...\n","Ground truth annotations: 344\n","Predicted detections: 17\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Adaptive Thresholding...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","Ground truth annotations: 344\n","Predicted detections: 1872\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running w/o Data Augmentation...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","Ground truth annotations: 344\n","Predicted detections: 15\n","loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running Larger Model...\n","Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8m.pt to 'yolov8m.pt'...\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 49.7M/49.7M [00:00<00:00, 405MB/s]\n"]},{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","YOLOv8m summary (fused): 92 layers, 25,886,080 parameters, 0 gradients, 78.9 GFLOPs\n","Ground truth annotations: 344\n","Predicted detections: 54\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","Running Lower Conf Threshold...\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","Ultralytics 8.3.165 🚀 Python-3.11.13 torch-2.6.0+cu124 CPU (Intel Xeon 2.20GHz)\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n","Ground truth annotations: 344\n","Predicted detections: 52\n","loading annotations into memory...\n","Done (t=0.00s)\n","creating index...\n","index created!\n","Error in compute_metrics: 'info'\n","\n","Ablation Study Results:\n","            Configuration  Accuracy (%)  Precision (%)  Recall (%)  F1-Score (%)  mAP (%)  MAE\n","              Full System         14.21          90.00        2.62          5.08      0.0 2.13\n","       w/o Multithreading         13.72          76.92        2.91          5.60      0.0 2.13\n","w/o Adaptive Thresholding         15.71          85.79      100.00         92.35      0.0 3.26\n","    w/o Data Augmentation         14.46          90.00        2.62          5.08      0.0 2.12\n","             Larger Model         14.96          78.05        9.30         16.62      0.0 2.08\n","     Lower Conf Threshold         14.96          87.50       10.17         18.23      0.0 2.07\n"]}]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
